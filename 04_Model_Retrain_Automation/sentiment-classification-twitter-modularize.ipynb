{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d13e37d",
   "metadata": {},
   "source": [
    "# Using Sagemaker ScriptProcessors and Estimators\n",
    "### Preprocess data and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089fce8",
   "metadata": {},
   "source": [
    "![](images/Processing-1.png)\n",
    "\n",
    "[Source](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697b106",
   "metadata": {},
   "source": [
    "### 0.0 Initial check/setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4516b",
   "metadata": {},
   "source": [
    "![](images/Terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ba984",
   "metadata": {},
   "source": [
    "```bash\n",
    "# After uploading the sentiment-modularized.tar.gz file to your Sagemaker notebook instance,\n",
    "# open a Terminal in JupyterLab and unzip the tar.gz file using the command below\n",
    "\n",
    "tar -xvf sentiment-modularized.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e481403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./dummy.log to s3://dsml-chrisi-bucket/athena_log/dummy.log\n"
     ]
    }
   ],
   "source": [
    "# Type your bucket name and run this cell\n",
    "\n",
    "BUCKET = 'dsml-chrisi-bucket'\n",
    "!aws s3 cp dummy.log s3://{BUCKET}/athena_log/dummy.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b96f8",
   "metadata": {},
   "source": [
    "### 1.0 Get data via PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c929fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyathena in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.2)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.21.25)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (8.0.1)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.24.25)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->pyathena) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666fc4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id dp_unique_key      target  \\\n",
      "0   2579    uq_id_1010  Irrelevant   \n",
      "1  12947   uq_id_10146     Neutral   \n",
      "2  12959   uq_id_10217    Negative   \n",
      "3  12964   uq_id_10246    Positive   \n",
      "4  13007   uq_id_10501    Positive   \n",
      "\n",
      "                                                text updated_date  \\\n",
      "0  I had to repaint another gun for Tiny Tina's c...   21-03-2022   \n",
      "1  Nvidia’s RTX 3080 is more exciting than PlaySt...   21-03-2022   \n",
      "2                                                the   21-03-2022   \n",
      "3                   This price is simply incredible.   21-03-2022   \n",
      "4                                        Can't wait!   21-03-2022   \n",
      "\n",
      "          entity  \n",
      "0    Borderlands  \n",
      "1  Xbox(Xseries)  \n",
      "2  Xbox(Xseries)  \n",
      "3  Xbox(Xseries)  \n",
      "4  Xbox(Xseries)  \n"
     ]
    }
   ],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "conn = connect(s3_staging_dir=\"s3://{BUCKET}/athena_log/ \", \n",
    "               region_name=\"us-west-2\")\n",
    "df = pd.read_sql_query(\"\"\" SELECT * FROM \"ml-workshop-db\".\"enriched_data\" \"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ffbdd",
   "metadata": {},
   "source": [
    "### 1.1 Load DataSet and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32c04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# rename columns\n",
    "df.columns = ['tweet_id', 'dp_unique_key', 'sentiment', 'tweet_text', 'updated_date', 'entity']\n",
    "\n",
    "#Define the indexing for each possible label in a dictionary\n",
    "class_to_index = {\"Neutral\":0, \"Irrelevant\":1, \"Negative\":2, \"Positive\": 3}\n",
    "\n",
    "#Creates a reverse dictionary\n",
    "index_to_class = dict((v,k) for k, v in class_to_index.items())\n",
    "\n",
    "#Creates lambda functions, applying the appropriate dictionary\n",
    "names_to_ids = lambda n: np.array([class_to_index.get(x) for x in n])\n",
    "ids_to_names = lambda n: np.array([index_to_class.get(x) for x in n])\n",
    "\n",
    "#Convert the \"Sentiment\" column into indexes\n",
    "df[\"sentiment_index\"] = names_to_ids(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe29b2",
   "metadata": {},
   "source": [
    "### 1.2 Look at dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5efd4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>dp_unique_key</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74675</th>\n",
       "      <td>9475</td>\n",
       "      <td>uq_id_8603</td>\n",
       "      <td>Negative</td>\n",
       "      <td>it myself and its @PlayOverwatch my fault. Fuc...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9524</td>\n",
       "      <td>uq_id_8885</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>LEGO probably won’t have anything for the Over...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>12868</td>\n",
       "      <td>uq_id_9682</td>\n",
       "      <td>Positive</td>\n",
       "      <td>£28. 99 won a month for an Xbox series 1 x eng...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>12901</td>\n",
       "      <td>uq_id_9876</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Best upcoming Xbox Series X games: The top nex...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>12906</td>\n",
       "      <td>uq_id_9906</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Crazy hyped to pre-order my Series X!! .</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id dp_unique_key   sentiment  \\\n",
       "74675      9475    uq_id_8603    Negative   \n",
       "74676      9524    uq_id_8885  Irrelevant   \n",
       "74677     12868    uq_id_9682    Positive   \n",
       "74678     12901    uq_id_9876     Neutral   \n",
       "74679     12906    uq_id_9906    Positive   \n",
       "\n",
       "                                              tweet_text updated_date  \\\n",
       "74675  it myself and its @PlayOverwatch my fault. Fuc...   21-03-2022   \n",
       "74676  LEGO probably won’t have anything for the Over...   21-03-2022   \n",
       "74677  £28. 99 won a month for an Xbox series 1 x eng...   21-03-2022   \n",
       "74678  Best upcoming Xbox Series X games: The top nex...   21-03-2022   \n",
       "74679           Crazy hyped to pre-order my Series X!! .   21-03-2022   \n",
       "\n",
       "              entity  sentiment_index  \n",
       "74675      Overwatch                2  \n",
       "74676      Overwatch                1  \n",
       "74677  Xbox(Xseries)                3  \n",
       "74678  Xbox(Xseries)                0  \n",
       "74679  Xbox(Xseries)                3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26efab8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      22542\n",
       "Positive      20830\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3858b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74680 entries, 0 to 74679\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   tweet_id         74680 non-null  int64 \n",
      " 1   dp_unique_key    74680 non-null  object\n",
      " 2   sentiment        74680 non-null  object\n",
      " 3   tweet_text       74680 non-null  object\n",
      " 4   updated_date     74680 non-null  object\n",
      " 5   entity           74680 non-null  object\n",
      " 6   sentiment_index  74680 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b2231",
   "metadata": {},
   "source": [
    "### 1.3 Filter out rows where tweet_text is NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11004afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiter data where tweet is present\n",
    "df = df[df.tweet_text.isnull()==False]\n",
    "df['tweet_text'] = df['tweet_text'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2de34",
   "metadata": {},
   "source": [
    "### 1.4 Stratified Sampling - For this session purposes lets restrict to 5000 records for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29032ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_each_class = 2000\n",
    "dfs_list = []\n",
    "for unique_sentiment in np.unique(df.sentiment):\n",
    "    df_sentiment = df[df.sentiment == unique_sentiment].sample(n=number_of_rows_each_class, random_state = 42)\n",
    "    dfs_list.append(df_sentiment)\n",
    "df = pd.concat(dfs_list)\n",
    "df = df.sample(frac=1, random_state = 42)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0dc0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      2000\n",
       "Irrelevant    2000\n",
       "Neutral       2000\n",
       "Positive      2000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f7eb1",
   "metadata": {},
   "source": [
    "### (Skip wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cda08",
   "metadata": {},
   "source": [
    "### 2.0 Prepare dataset for modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdbf43",
   "metadata": {},
   "source": [
    "```python\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet_text):\n",
    "    tweet_text = re.sub('[^a-zA-Z]', ' ', tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = tweet_text.split()\n",
    "    tweet_text = [lemmatizer.lemmatize(word) for word in tweet_text if (not(word in set(stopwords))) & (len(word)>1) ]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "df['tweet_text_preprocessed'] = df['tweet_text'].progress_apply(preprocess_tweet)\n",
    "df = df[df.tweet_text_preprocessed.apply(lambda text: len(text.split())>1)]\n",
    "#df['tweet_tokenized'] = df['tweet_tokenized'].progress_apply(lemmatize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e007c",
   "metadata": {},
   "source": [
    "### 2.1 Save dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f463685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61be7cb",
   "metadata": {},
   "source": [
    "### 2.2 Write code above into script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93a62397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting process_tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile process_tweets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "install('nltk')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet_text):\n",
    "    tweet_text = re.sub('[^a-zA-Z]', ' ', tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = tweet_text.split()\n",
    "    tweet_text = [lemmatizer.lemmatize(word) for word in tweet_text if not word in set(stopwords)]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['tweet_text_preprocessed'] = df.apply(lambda x: preprocess_tweet(x['tweet_text']), axis=1)\n",
    "    df = df[df.tweet_text_preprocessed.apply(lambda text: len(text.split())>1)]\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = os.path.join('/opt/ml/processing/input', 'tweets.csv')\n",
    "    output_file = os.path.join('/opt/ml/processing/output', 'tweets_processed.csv')\n",
    "    #input_file = os.path.join('.', 'tweets.csv')\n",
    "    #output_file = os.path.join('.', 'tweets_processed.csv')\n",
    "    \n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca6876",
   "metadata": {},
   "source": [
    "### 2.3 Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f1600b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python process_tweets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "255aa04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_text_preprocessed</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im sorry but valorant looks such an exact flip...</td>\n",
       "      <td>im sorry valorant look exact flip unity game h...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow, I no longer google photos, now I am sad a...</td>\n",
       "      <td>wow longer google photo sad</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kena is Easily another of my top 5 most antici...</td>\n",
       "      <td>kena easily another top anticipated</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bitch i did not play call of duty all these ye...</td>\n",
       "      <td>bitch play call duty year back got call duty y...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just want you all to know that you are loved...</td>\n",
       "      <td>want know loved besides live something twitch ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And bro what?. lmaooo may ima be been mad as hell</td>\n",
       "      <td>bro lmaooo may ima mad hell</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Xbox office : E3 IS CANCELLED, AND WHAT ARE WE...</td>\n",
       "      <td>xbox office e cancelled gonna sl show new cons...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A lot of dumb people in the world giving out a</td>\n",
       "      <td>lot dumb people world giving</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When you have to leave your teammate because i...</td>\n",
       "      <td>leave teammate ranked playapex</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microsoft Azure fell today, probably why.</td>\n",
       "      <td>microsoft azure fell today probably</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gonna jump ahead will say this is a Clip-and-S...</td>\n",
       "      <td>gonna jump ahead say clip save piece content b...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Smh this is in NASTY</td>\n",
       "      <td>smh nasty</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@EAHelp I just ordered 19:99 and fifa points 2...</td>\n",
       "      <td>eahelp ordered fifa point showing</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Watch StrapxAxHolic on Twitch! But I think tha...</td>\n",
       "      <td>watch strapxaxholic twitch think pretty fuckin...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VACCINE PAUSED: Johnson &amp; Johnson further paus...</td>\n",
       "      <td>vaccine paused johnson johnson paused covid va...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Today’s Best Game Deals: FIFA 20 $18, Borderla...</td>\n",
       "      <td>today best game deal fifa borderland handsome ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@ Rainbow6Game Feels good when you get a full ...</td>\n",
       "      <td>rainbow game feel good get full stack win game...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Threat neutralized</td>\n",
       "      <td>threat neutralized</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This FUCKING HURT to read, I can't wait for Se...</td>\n",
       "      <td>fucking hurt read wait september bro</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hey @VZWSupport – response?</td>\n",
       "      <td>hey vzwsupport response</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Thousands of lawsuits have been filed against ...</td>\n",
       "      <td>thousand lawsuit filed django pharmaceutical p...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cowboy Ursa Major died for this</td>\n",
       "      <td>cowboy ursa major died</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HOME DEPOT Blessed!!!!!!!!!!!!!</td>\n",
       "      <td>home depot blessed</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Best game ever!!. @ MrJCorbs @ PlayApex pic.fm...</td>\n",
       "      <td>best game ever mrjcorbs playapex pic fm exzb ggk</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>if you go to the middle of Baghdad for any rea...</td>\n",
       "      <td>go middle baghdad reason trickshot</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dead ass bro.</td>\n",
       "      <td>dead as bro</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>funny, did laugh thank you blue fortnite though</td>\n",
       "      <td>funny laugh thank blue fortnite though</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Can't seem to connect to any ranked matche reg...</td>\n",
       "      <td>seem connect ranked matche region sea ubisoft ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The ban for Battlefield 4 911 _ kbbbbbbb.com p...</td>\n",
       "      <td>ban battlefield kbbbbbbb com player due detail...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Am I actually the only adult one experiencing ...</td>\n",
       "      <td>actually adult one experiencing many issue bug...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   im sorry but valorant looks such an exact flip...   \n",
       "1   Wow, I no longer google photos, now I am sad a...   \n",
       "2   Kena is Easily another of my top 5 most antici...   \n",
       "3   bitch i did not play call of duty all these ye...   \n",
       "4   I just want you all to know that you are loved...   \n",
       "5   And bro what?. lmaooo may ima be been mad as hell   \n",
       "6   Xbox office : E3 IS CANCELLED, AND WHAT ARE WE...   \n",
       "7      A lot of dumb people in the world giving out a   \n",
       "8   When you have to leave your teammate because i...   \n",
       "9           Microsoft Azure fell today, probably why.   \n",
       "10  Gonna jump ahead will say this is a Clip-and-S...   \n",
       "11                               Smh this is in NASTY   \n",
       "12  @EAHelp I just ordered 19:99 and fifa points 2...   \n",
       "13  Watch StrapxAxHolic on Twitch! But I think tha...   \n",
       "14  VACCINE PAUSED: Johnson & Johnson further paus...   \n",
       "15  Today’s Best Game Deals: FIFA 20 $18, Borderla...   \n",
       "16  @ Rainbow6Game Feels good when you get a full ...   \n",
       "17                                 Threat neutralized   \n",
       "18  This FUCKING HURT to read, I can't wait for Se...   \n",
       "19                        Hey @VZWSupport – response?   \n",
       "20  Thousands of lawsuits have been filed against ...   \n",
       "21                    Cowboy Ursa Major died for this   \n",
       "22                    HOME DEPOT Blessed!!!!!!!!!!!!!   \n",
       "23  Best game ever!!. @ MrJCorbs @ PlayApex pic.fm...   \n",
       "24  if you go to the middle of Baghdad for any rea...   \n",
       "25                                      Dead ass bro.   \n",
       "26    funny, did laugh thank you blue fortnite though   \n",
       "27  Can't seem to connect to any ranked matche reg...   \n",
       "28  The ban for Battlefield 4 911 _ kbbbbbbb.com p...   \n",
       "29  Am I actually the only adult one experiencing ...   \n",
       "\n",
       "                              tweet_text_preprocessed   sentiment  \n",
       "0   im sorry valorant look exact flip unity game h...    Negative  \n",
       "1                         wow longer google photo sad    Negative  \n",
       "2                 kena easily another top anticipated  Irrelevant  \n",
       "3   bitch play call duty year back got call duty y...    Negative  \n",
       "4   want know loved besides live something twitch ...     Neutral  \n",
       "5                         bro lmaooo may ima mad hell    Negative  \n",
       "6   xbox office e cancelled gonna sl show new cons...  Irrelevant  \n",
       "7                        lot dumb people world giving  Irrelevant  \n",
       "8                      leave teammate ranked playapex     Neutral  \n",
       "9                 microsoft azure fell today probably    Negative  \n",
       "10  gonna jump ahead say clip save piece content b...  Irrelevant  \n",
       "11                                          smh nasty    Negative  \n",
       "12                  eahelp ordered fifa point showing    Negative  \n",
       "13  watch strapxaxholic twitch think pretty fuckin...     Neutral  \n",
       "14  vaccine paused johnson johnson paused covid va...     Neutral  \n",
       "15  today best game deal fifa borderland handsome ...     Neutral  \n",
       "16  rainbow game feel good get full stack win game...    Positive  \n",
       "17                                 threat neutralized     Neutral  \n",
       "18               fucking hurt read wait september bro    Negative  \n",
       "19                            hey vzwsupport response    Negative  \n",
       "20  thousand lawsuit filed django pharmaceutical p...    Negative  \n",
       "21                             cowboy ursa major died     Neutral  \n",
       "22                                 home depot blessed    Positive  \n",
       "23   best game ever mrjcorbs playapex pic fm exzb ggk    Positive  \n",
       "24                 go middle baghdad reason trickshot  Irrelevant  \n",
       "25                                        dead as bro    Negative  \n",
       "26             funny laugh thank blue fortnite though    Positive  \n",
       "27  seem connect ranked matche region sea ubisoft ...    Negative  \n",
       "28  ban battlefield kbbbbbbb com player due detail...  Irrelevant  \n",
       "29  actually adult one experiencing many issue bug...    Negative  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('tweets_processed.csv')[['tweet_text','tweet_text_preprocessed', 'sentiment']].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c1ac9",
   "metadata": {},
   "source": [
    "### 2.4 Initialize Sagemaker ScriptProcessor (SKLearnProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6fc2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac56a3",
   "metadata": {},
   "source": [
    "### 2.5 (TODO) Create S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c37c1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08738e1a",
   "metadata": {},
   "source": [
    "### 2.6 Copy tweets.csv to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a079ec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./tweets.csv to s3://dsml-chrisi-bucket/workshop/tweets.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tweets.csv s3://dsml-chrisi-bucket/workshop/tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab3d6d",
   "metadata": {},
   "source": [
    "### 2.7 Run ScriptProcessor\n",
    "IMPORTANT: Uncomment/comment lines in `process_tweets.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "986f82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2022-04-01-21-17-02-136\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://dsml-chrisi-bucket/workshop/tweets.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-995383923238/sagemaker-scikit-learn-2022-04-01-21-17-02-136/input/code/process_tweets.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'tweet_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://dsml-chrisi-bucket/workshop', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........................\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /miniconda3/lib/python3.7/site-packages (from nltk) (4.62.3)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /miniconda3/lib/python3.7/site-packages (from nltk) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /miniconda3/lib/python3.7/site-packages (from click->nltk) (4.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, joblib, nltk\u001b[0m\n",
      "\u001b[34mSuccessfully installed joblib-1.1.0 nltk-3.7 regex-2022.3.15\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package wordnet to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/wordnet.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package stopwords to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/stopwords.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/omw-1.4.zip.\u001b[0m\n",
      "\n",
      "CPU times: user 652 ms, sys: 18 ms, total: 670 ms\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='process_tweets.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source='s3://dsml-chrisi-bucket/workshop/tweets.csv',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='tweet_output',\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination='s3://dsml-chrisi-bucket/workshop'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093ba38",
   "metadata": {},
   "source": [
    "### 2.8 Open a new browser tab and check your running job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c21464",
   "metadata": {},
   "source": [
    "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/processing-jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30988a8b",
   "metadata": {},
   "source": [
    "### 2.9 View output in S3 bucket\n",
    "You should see a new file named `tweets_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b517e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 04:37:27    1299233 tweets.csv\n",
      "2022-03-31 04:41:25    1830178 tweets_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9883203",
   "metadata": {},
   "source": [
    "### RECAP #1\n",
    "1. Saved dataframe to a csv file in S3 (2.1)\n",
    "2. Transferred notebook code into Python script file (2.2)\n",
    "3. Used Sagemaker ScriptProcessor to run our script in a container and process the files to/from S3 (2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4da4b",
   "metadata": {},
   "source": [
    "### 3.0 Split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f827c1",
   "metadata": {},
   "source": [
    "```python\n",
    "train_df, test_df = train_test_split(df,test_size = 0.05, random_state =42)\n",
    "\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_train = train_df['tweet_text_preprocessed']\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "X_test = test_df['tweet_text_preprocessed']\n",
    "y_test = test_df['sentiment']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c60eeb",
   "metadata": {},
   "source": [
    "### 3.1 Keras tokenization word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555f71f",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "\n",
    "keras_tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "\n",
    "\n",
    "def tokenize_pad_sequences(tweet_text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    tweet_text = tokenizer.texts_to_sequences(tweet_text)\n",
    "    # Pad sequences to the same length\n",
    "    tweet_text = pad_sequences(tweet_text, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return tweet_text\n",
    "\n",
    "keras_tokenizer.fit_on_texts(train_df['tweet_text_preprocessed'])\n",
    "train_texts_to_sequences = keras_tokenizer.texts_to_sequences(train_df['tweet_text_preprocessed'])\n",
    "train_texts_to_sequences = pad_sequences(train_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
    "\n",
    "\n",
    "test_texts_to_sequences = keras_tokenizer.texts_to_sequences(test_df['tweet_text_preprocessed'])\n",
    "test_texts_to_sequences = pad_sequences(test_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
    "\n",
    "\n",
    "# saving\n",
    "# with open('keras_tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(keras_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "tokenizer_json =  keras_tokenizer.to_json()\n",
    "with io.open(f'{current_path}/keras_model_files/keras_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd8c03",
   "metadata": {},
   "source": [
    "### 3.2 Write above code into training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "defa23a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing keras_train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile keras_train_model.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model\n",
    "#from keras.models import save_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "keras_tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "\n",
    "\n",
    "def train(input_file, output_folder):\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df,test_size = 0.05, random_state =42)\n",
    "\n",
    "    train_df.reset_index(drop = True, inplace = True)\n",
    "    test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    X_train = train_df['tweet_text_preprocessed']\n",
    "    y_train = train_df['sentiment']\n",
    "\n",
    "    X_test = test_df['tweet_text_preprocessed']\n",
    "    y_test = test_df['sentiment']\n",
    "\n",
    "    keras_tokenizer.fit_on_texts(train_df['tweet_text_preprocessed'])\n",
    "    train_texts_to_sequences = keras_tokenizer.texts_to_sequences(train_df['tweet_text_preprocessed'])\n",
    "    train_texts_to_sequences = pad_sequences(train_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "    train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
    "\n",
    "    test_texts_to_sequences = keras_tokenizer.texts_to_sequences(test_df['tweet_text_preprocessed'])\n",
    "    test_texts_to_sequences = pad_sequences(test_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "    test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    embedding_vector_size = 16\n",
    "    lstm_units = 20\n",
    "    keras_model.add(Embedding(max_words,embedding_vector_size,input_length=max_len))\n",
    "    #keras_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "    keras_model.add(Bidirectional(LSTM(lstm_units)))\n",
    "    keras_model.add(Dense(4, activation='softmax'))\n",
    "    keras_model.compile(\n",
    "         loss='sparse_categorical_crossentropy',\n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "    \n",
    "    X_train_keras = train_texts_to_sequences\n",
    "    y_train_keras = train_df['sentiment_index']\n",
    "\n",
    "    X_test_keras = test_texts_to_sequences\n",
    "    y_test_keras = test_df['sentiment_index']\n",
    "\n",
    "    keras_model.fit(\n",
    "         X_train_keras, y_train_keras,\n",
    "         validation_data=(X_test_keras, y_test_keras),\n",
    "         epochs=1)\n",
    "    \n",
    "    save_model(keras_model, output_folder.rstrip('/') + '/', save_format='tf')\n",
    "\n",
    "    print('DONE')\n",
    "    #tokenizer_json =  keras_tokenizer.to_json()\n",
    "    #with io.open(f'{current_path}/keras_model_files/keras_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    #    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = os.environ.get('SM_CHANNEL_TRAIN') + '/tweets_processed.csv'\n",
    "    output_folder = os.environ.get('SM_MODEL_DIR') #os.path.join('/opt/ml/processing/output', 'tweets_processed.csv')\n",
    "\n",
    "    #input_file = os.path.join('.', 'tweets_processed.csv')\n",
    "    #output_folder = '.'\n",
    "    \n",
    "    print('Input', input_file)\n",
    "    print('Output', output_folder)\n",
    "       \n",
    "    train(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb58d2a",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Sagemaker Estimator\n",
    "Let's compare with (2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17b3b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecacf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add link to mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3a819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "                 entry_point='keras_train_model.py',\n",
    "                 instance_type='ml.p3.2xlarge', #'ml.p3.8xlarge', 'local'\n",
    "                 instance_count=1,\n",
    "                 source_dir='.',\n",
    "                 role=role,\n",
    "                 framework_version='2.3.2',\n",
    "                 py_version='py37',\n",
    "                 output_path='s3://dsml-chrisi-bucket/workshop',\n",
    "                 hyperparameters={\n",
    "                     #'embedding': True,\n",
    "                     #'modelstart': 1,\n",
    "                     #'batch-size': 64,\n",
    "                     #'modelfinish': 5\n",
    "                 },\n",
    "                 #script_mode=True,\n",
    "                 #dependencies=dependencies,\n",
    "                 #image_uri=<image_uri>,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7850615",
   "metadata": {},
   "source": [
    "### 3.4 Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d02d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 05:07:07 Starting - Starting the training job...\n",
      "2022-03-31 05:07:33 Starting - Preparing the instances for trainingProfilerReport-1648703227: InProgress\n",
      ".........\n",
      "2022-03-31 05:09:06 Downloading - Downloading input data\n",
      "2022-03-31 05:09:06 Training - Downloading the training image...............\n",
      "2022-03-31 05:11:31 Training - Training image download completed. Training in progress.\u001b[34m2022-03-31 05:11:27.096939: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:27.103660: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:27.348119: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:27.437188: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:31,402 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:32,287 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tweets-chris04\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://dsml-chrisi-bucket/tweets-chris04/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"keras_train_model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"keras_train_model.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=keras_train_model.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=keras_train_model\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://dsml-chrisi-bucket/tweets-chris04/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tweets-chris04\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://dsml-chrisi-bucket/tweets-chris04/source/sourcedir.tar.gz\",\"module_name\":\"keras_train_model\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"keras_train_model.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 keras_train_model.py --model_dir s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\u001b[0m\n",
      "\u001b[34mInput /opt/ml/input/data/train/tweets_processed.csv\u001b[0m\n",
      "\u001b[34mOutput /opt/ml/model\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:40.354 ip-10-0-198-174.us-west-2.compute.internal:35 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:40.741 ip-10-0-198-174.us-west-2.compute.internal:35 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.531 ip-10-0-198-174.us-west-2.compute.internal:35 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.531 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.532 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.532 ip-10-0-198-174.us-west-2.compute.internal:35 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.533 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:425] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.536 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:425] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m#015  1/221 [..............................] - ETA: 0s - loss: 1.3868 - accuracy: 0.2812 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  6/221 [..............................] - ETA: 2s - loss: 1.3863 - accuracy: 0.2448 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 11/221 [>.............................] - ETA: 2s - loss: 1.3864 - accuracy: 0.2301 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 17/221 [=>............................] - ETA: 2s - loss: 1.3860 - accuracy: 0.2445 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 23/221 [==>...........................] - ETA: 1s - loss: 1.3850 - accuracy: 0.2704 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 28/221 [==>...........................] - ETA: 1s - loss: 1.3844 - accuracy: 0.2690 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 33/221 [===>..........................] - ETA: 1s - loss: 1.3830 - accuracy: 0.2794 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 38/221 [====>.........................] - ETA: 1s - loss: 1.3841 - accuracy: 0.2722 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 43/221 [====>.........................] - ETA: 1s - loss: 1.3846 - accuracy: 0.2638 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 48/221 [=====>........................] - ETA: 1s - loss: 1.3844 - accuracy: 0.2643 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 53/221 [======>.......................] - ETA: 1s - loss: 1.3842 - accuracy: 0.2618 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 58/221 [======>.......................] - ETA: 1s - loss: 1.3842 - accuracy: 0.2619 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 63/221 [=======>......................] - ETA: 1s - loss: 1.3838 - accuracy: 0.2659 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 67/221 [========>.....................] - ETA: 1s - loss: 1.3833 - accuracy: 0.2654 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 72/221 [========>.....................] - ETA: 1s - loss: 1.3831 - accuracy: 0.2639 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 77/221 [=========>....................] - ETA: 1s - loss: 1.3831 - accuracy: 0.2662 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 82/221 [==========>...................] - ETA: 1s - loss: 1.3826 - accuracy: 0.2717 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 88/221 [==========>...................] - ETA: 1s - loss: 1.3818 - accuracy: 0.2763 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 93/221 [===========>..................] - ETA: 1s - loss: 1.3813 - accuracy: 0.2802 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 99/221 [============>.................] - ETA: 1s - loss: 1.3808 - accuracy: 0.2869 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015104/221 [=============>................] - ETA: 1s - loss: 1.3803 - accuracy: 0.2900 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015109/221 [=============>................] - ETA: 1s - loss: 1.3793 - accuracy: 0.2924 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015114/221 [==============>...............] - ETA: 1s - loss: 1.3787 - accuracy: 0.2966 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015119/221 [===============>..............] - ETA: 1s - loss: 1.3774 - accuracy: 0.2994 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015124/221 [===============>..............] - ETA: 1s - loss: 1.3765 - accuracy: 0.3012 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015129/221 [================>.............] - ETA: 0s - loss: 1.3753 - accuracy: 0.3067 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015135/221 [=================>............] - ETA: 0s - loss: 1.3745 - accuracy: 0.3083 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015141/221 [==================>...........] - ETA: 0s - loss: 1.3730 - accuracy: 0.3107 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015147/221 [==================>...........] - ETA: 0s - loss: 1.3708 - accuracy: 0.3163 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015152/221 [===================>..........] - ETA: 0s - loss: 1.3690 - accuracy: 0.3207 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015157/221 [====================>.........] - ETA: 0s - loss: 1.3666 - accuracy: 0.3244 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015162/221 [====================>.........] - ETA: 0s - loss: 1.3629 - accuracy: 0.3285 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015168/221 [=====================>........] - ETA: 0s - loss: 1.3616 - accuracy: 0.3322 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015173/221 [======================>.......] - ETA: 0s - loss: 1.3594 - accuracy: 0.3363 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015179/221 [=======================>......] - ETA: 0s - loss: 1.3552 - accuracy: 0.3417 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015184/221 [=======================>......] - ETA: 0s - loss: 1.3528 - accuracy: 0.3448 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015189/221 [========================>.....] - ETA: 0s - loss: 1.3504 - accuracy: 0.3492 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015194/221 [=========================>....] - ETA: 0s - loss: 1.3484 - accuracy: 0.3507 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015200/221 [==========================>...] - ETA: 0s - loss: 1.3447 - accuracy: 0.3531 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015206/221 [==========================>...] - ETA: 0s - loss: 1.3412 - accuracy: 0.3565 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015211/221 [===========================>..] - ETA: 0s - loss: 1.3383 - accuracy: 0.3589 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015216/221 [============================>.] - ETA: 0s - loss: 1.3370 - accuracy: 0.3600 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015221/221 [==============================] - ETA: 0s - loss: 1.3332 - accuracy: 0.3630 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015221/221 [==============================] - 3s 15ms/step - loss: 1.3332 - accuracy: 0.3630 - batch: 0.0000e+00 - val_loss: 1.2142 - val_accuracy: 0.4853\u001b[0m\n",
      "\u001b[34mDONE\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:33.797253: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:33.797418: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:33.842099: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mkeras_train_model.py:42: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\u001b[0m\n",
      "\u001b[34mkeras_train_model.py:46: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:57.606515: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34m2022-03-31 05:12:07,207 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving containers. The SavedModel bundle is under directory \"model\", not a numeric name.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:12:07,208 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-03-31 05:12:31 Uploading - Uploading generated training model\n",
      "2022-03-31 05:12:31 Completed - Training job completed\n",
      "Training seconds: 218\n",
      "Billable seconds: 218\n",
      "CPU times: user 1.29 s, sys: 51.2 ms, total: 1.34 s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({\n",
    "    'train': 's3://dsml-chrisi-bucket/workshop/',\n",
    "}, job_name='tweets-chris04')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9090539",
   "metadata": {},
   "source": [
    "### 3.5 Open a new browser tab and check your training job\n",
    "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5318ece",
   "metadata": {},
   "source": [
    "### 3.6 View output in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a56b583f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE tweets-chris04/\n",
      "2022-03-31 04:37:27    1299233 tweets.csv\n",
      "2022-03-31 04:41:25    1830178 tweets_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a107f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE debug-output/\n",
      "                           PRE output/\n",
      "                           PRE profiler-output/\n",
      "                           PRE rule-output/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/tweets-chris04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1a835db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 05:12:17    1224576 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/tweets-chris04/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c7f40",
   "metadata": {},
   "source": [
    "### 3.7 Copy model.tar.gz to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5581a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dsml-chrisi-bucket/workshop/tweets-chris04/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://dsml-chrisi-bucket/workshop/tweets-chris04/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7f97a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets/\n",
      "saved_model.pb\n",
      "variables/\n",
      "variables/variables.index\n",
      "variables/variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!mkdir keras_model_files\n",
    "!tar -xvf model.tar.gz --directory ./keras_model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb107272",
   "metadata": {},
   "source": [
    "### RECAP #2\n",
    "1. Transferred notebook code into Python training script file (3.2)\n",
    "2. Used Sagemaker Estimator to run our script in a container and process the files to/from S3 (3.3 - 3.4)\n",
    "3. Copied back the Estimator output from S3 to local (3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01b25a",
   "metadata": {},
   "source": [
    "### What next?\n",
    "- [Sagemaker notebook examples](https://github.com/aws/amazon-sagemaker-examples) - (accessible via the Sagemaker notebook extension on the lower left of this Jupyterlab)\n",
    "- [Data Science on AWS (Book)](https://www.oreilly.com/library/view/data-science-on/9781492079385/) \n",
    "- Non-AWS\n",
    "  - [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/quickstart/)\n",
    "  - [MLFlow](https://mlflow.org/)\n",
    "  - [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-hello-world)\n",
    "  - Tensorflow Extended\n",
    "- Moar AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42579ae",
   "metadata": {},
   "source": [
    "![](images/AWLMLStack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b85004dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook sentiment-classification-twitter-modularize.ipynb to html\n",
      "[NbConvertApp] Writing 707574 bytes to sentiment-classification-twitter-modularize.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html sentiment-classification-twitter-modularize.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05ff750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    ".ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a320236",
   "metadata": {},
   "source": [
    "### Zip up files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56120695",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm sentiment-modularized.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50fb3cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./delete_resources.py\n",
      "./.gitignore\n",
      "./README.md\n",
      "./resources/\n",
      "./resources/process_tweets.py\n",
      "./resources/keras_train_model.py\n",
      "./resources/tweets.csv\n",
      "./resources/tweets_processed.csv\n",
      "./resources/model.tar.gz\n",
      "./resources/keras_model_files/\n",
      "./resources/keras_model_files/variables/\n",
      "./resources/keras_model_files/variables/variables.index\n",
      "./resources/keras_model_files/variables/variables.data-00000-of-00001\n",
      "./resources/keras_model_files/saved_model.pb\n",
      "./resources/keras_model_files/assets/\n",
      "./sentiment-classification-twitter-modularize.html\n",
      "./dummy.log\n",
      "./images/\n",
      "./images/Processing-1.png\n",
      "./images/Terminal.png\n",
      "./images/AWLMLStack.png\n",
      "./sentiment-classification-twitter-modularize.ipynb\n",
      "tar: .: file changed as we read it\n"
     ]
    }
   ],
   "source": [
    "!tar --exclude=\".ipynb_checkpoints*\" -zcvf sentiment-modularized.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec9d73",
   "metadata": {},
   "source": [
    "### Delete step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IAMRole\n",
    "Sagemaker notebook instance\n",
    "S3 bucket\n",
    "Athena table (3x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
