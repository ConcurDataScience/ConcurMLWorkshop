{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8189119",
   "metadata": {},
   "source": [
    "# Using Sagemaker ScriptProcessor and Estimator\n",
    "### Preprocess data and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d8df9",
   "metadata": {},
   "source": [
    "![](images/Processing-1.png)\n",
    "\n",
    "[Source](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9779e8",
   "metadata": {},
   "source": [
    "### 1.0 Get data via PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5408af49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyathena\n",
      "  Downloading PyAthena-2.3.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.24.25)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (8.0.1)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.21.25)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->pyathena) (1.15.0)\n",
      "Installing collected packages: pyathena\n",
      "Successfully installed pyathena-2.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3a62ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id dp_unique_key      target  \\\n",
      "0  12934   uq_id_10078     Neutral   \n",
      "1  12945   uq_id_10136     Neutral   \n",
      "2  12983   uq_id_10356  Irrelevant   \n",
      "3  12999   uq_id_10456    Positive   \n",
      "4  13009   uq_id_10514    Negative   \n",
      "\n",
      "                                                text updated_date  \\\n",
      "0  Xbox Edition Series Super X WINS Up Again! | a...   21-03-2022   \n",
      "1  Price delay for PS5 and Xbox Series X: more BA...   21-03-2022   \n",
      "2  If you haven't gotten Game Pass yet, you serio...   21-03-2022   \n",
      "3  _.. The faster to more energy efficient AMD Ze...   21-03-2022   \n",
      "4  @ IdleSloth1984, what the hell do you mean? Xb...   21-03-2022   \n",
      "\n",
      "          entity  \n",
      "0  Xbox(Xseries)  \n",
      "1  Xbox(Xseries)  \n",
      "2  Xbox(Xseries)  \n",
      "3  Xbox(Xseries)  \n",
      "4  Xbox(Xseries)  \n"
     ]
    }
   ],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "conn = connect(s3_staging_dir=\"s3://aws-athena-query-results-us-west-2-995383923238/ \",\n",
    "               region_name=\"us-west-2\")\n",
    "df = pd.read_sql_query(\"\"\" SELECT * FROM \"ml-workshop-db\".\"enriched_data\" \"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa184d",
   "metadata": {},
   "source": [
    "### 1.1 Load DataSet and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d628e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# rename columns\n",
    "df.columns = ['tweet_id', 'dp_unique_key', 'sentiment', 'tweet_text', 'updated_date', 'entity']\n",
    "\n",
    "#Define the indexing for each possible label in a dictionary\n",
    "class_to_index = {\"Neutral\":0, \"Irrelevant\":1, \"Negative\":2, \"Positive\": 3}\n",
    "\n",
    "#Creates a reverse dictionary\n",
    "index_to_class = dict((v,k) for k, v in class_to_index.items())\n",
    "\n",
    "#Creates lambda functions, applying the appropriate dictionary\n",
    "names_to_ids = lambda n: np.array([class_to_index.get(x) for x in n])\n",
    "ids_to_names = lambda n: np.array([index_to_class.get(x) for x in n])\n",
    "\n",
    "#Convert the \"Sentiment\" column into indexes\n",
    "df[\"sentiment_index\"] = names_to_ids(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7b48e",
   "metadata": {},
   "source": [
    "### 1.2 Look at dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf1a754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>dp_unique_key</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74675</th>\n",
       "      <td>9589</td>\n",
       "      <td>uq_id_9244</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The fuck it. we doin overwatch twitch.tv/Nikkaela</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9599</td>\n",
       "      <td>uq_id_9300</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>My gorgeous and hilarious girlfriend is stream...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>12817</td>\n",
       "      <td>uq_id_9411</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Did n just have really high expectations for the</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>12826</td>\n",
       "      <td>uq_id_9461</td>\n",
       "      <td>Positive</td>\n",
       "      <td>i want gold.</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>2417</td>\n",
       "      <td>uq_id_99</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Grounded almost looked pretty cool here despit...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id dp_unique_key   sentiment  \\\n",
       "74675      9589    uq_id_9244     Neutral   \n",
       "74676      9599    uq_id_9300  Irrelevant   \n",
       "74677     12817    uq_id_9411    Negative   \n",
       "74678     12826    uq_id_9461    Positive   \n",
       "74679      2417      uq_id_99    Negative   \n",
       "\n",
       "                                              tweet_text updated_date  \\\n",
       "74675  The fuck it. we doin overwatch twitch.tv/Nikkaela   21-03-2022   \n",
       "74676  My gorgeous and hilarious girlfriend is stream...   21-03-2022   \n",
       "74677   Did n just have really high expectations for the   21-03-2022   \n",
       "74678                                       i want gold.   21-03-2022   \n",
       "74679  Grounded almost looked pretty cool here despit...   21-03-2022   \n",
       "\n",
       "              entity  sentiment_index  \n",
       "74675      Overwatch                0  \n",
       "74676      Overwatch                1  \n",
       "74677  Xbox(Xseries)                2  \n",
       "74678  Xbox(Xseries)                3  \n",
       "74679    Borderlands                2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c80b4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      22542\n",
       "Positive      20830\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5771a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74680 entries, 0 to 74679\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   tweet_id         74680 non-null  int64 \n",
      " 1   dp_unique_key    74680 non-null  object\n",
      " 2   sentiment        74680 non-null  object\n",
      " 3   tweet_text       74680 non-null  object\n",
      " 4   updated_date     74680 non-null  object\n",
      " 5   entity           74680 non-null  object\n",
      " 6   sentiment_index  74680 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8243b9",
   "metadata": {},
   "source": [
    "### 1.3 Filter out rows where tweet_text is NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0043e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiter data where tweet is present\n",
    "df = df[df.tweet_text.isnull()==False]\n",
    "df['tweet_text'] = df['tweet_text'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1efb8f",
   "metadata": {},
   "source": [
    "### 1.4 Stratified Sampling - For this session purposes lets restrict to 5000 records for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad888b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_each_class = 2000\n",
    "dfs_list = []\n",
    "for unique_sentiment in np.unique(df.sentiment):\n",
    "    df_sentiment = df[df.sentiment == unique_sentiment].sample(n=number_of_rows_each_class, random_state = 42)\n",
    "    dfs_list.append(df_sentiment)\n",
    "df = pd.concat(dfs_list)\n",
    "df = df.sample(frac=1, random_state = 42)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5df9a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive      2000\n",
       "Irrelevant    2000\n",
       "Negative      2000\n",
       "Neutral       2000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ebf05",
   "metadata": {},
   "source": [
    "### (Skip wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608f7f8",
   "metadata": {},
   "source": [
    "### 2.0 Prepare dataset for modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13af881",
   "metadata": {},
   "source": [
    "```python\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet_text):\n",
    "    tweet_text = re.sub('[^a-zA-Z]', ' ', tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = tweet_text.split()\n",
    "    tweet_text = [lemmatizer.lemmatize(word) for word in tweet_text if (not(word in set(stopwords))) & (len(word)>1) ]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "df['tweet_text_preprocessed'] = df['tweet_text'].progress_apply(preprocess_tweet)\n",
    "df = df[df.tweet_text_preprocessed.apply(lambda text: len(text.split())>1)]\n",
    "#df['tweet_tokenized'] = df['tweet_tokenized'].progress_apply(lemmatize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f02a3",
   "metadata": {},
   "source": [
    "### 2.1 Save dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9fc6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166604",
   "metadata": {},
   "source": [
    "### 2.2 Write code above into script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0462c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting process_tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile process_tweets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "install('nltk')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet_text):\n",
    "    tweet_text = re.sub('[^a-zA-Z]', ' ', tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = tweet_text.split()\n",
    "    tweet_text = [lemmatizer.lemmatize(word) for word in tweet_text if not word in set(stopwords)]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['tweet_text_preprocessed'] = df.apply(lambda x: preprocess_tweet(x['tweet_text']), axis=1)\n",
    "    df = df[df.tweet_text_preprocessed.apply(lambda text: len(text.split())>1)]\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = os.path.join('/opt/ml/processing/input', 'tweets.csv')\n",
    "    output_file = os.path.join('/opt/ml/processing/output', 'tweets_processed.csv')\n",
    "    #input_file = os.path.join('.', 'tweets.csv')\n",
    "    #output_file = os.path.join('.', 'tweets_processed.csv')\n",
    "    \n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183584a",
   "metadata": {},
   "source": [
    "### 2.3 Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3968f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python process_tweets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6364a2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_text_preprocessed</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m just gonna say it - Overwatch game being n...</td>\n",
       "      <td>gonna say overwatch game nominated lgbtq game ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A company that has the ability to design and p...</td>\n",
       "      <td>company ability design produce top line graphi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everything is better when it's black</td>\n",
       "      <td>everything better black</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RIP Battlefield V</td>\n",
       "      <td>rip battlefield v</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borderlands 3 Chapter 1 youtu.be / 0SKu6Vr4iXU...</td>\n",
       "      <td>borderland chapter youtu sku vr ixu via youtub...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>johnson &amp; johnson about a purchase COVID but s...</td>\n",
       "      <td>johnson johnson purchase covid still fix</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The World&lt;unk&gt; Warcraft category on play is so...</td>\n",
       "      <td>world unk warcraft category play sooo congeste...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@joem135... You know what's bloody stupid? Som...</td>\n",
       "      <td>joem know bloody stupid someone stupid profit ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from me is slightly disappointed being my sche...</td>\n",
       "      <td>slightly disappointed schedule explicitly appr...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@ Xfinity The Call of Duty: Black Ops Cold War...</td>\n",
       "      <td>xfinity call duty black ops cold war open beta...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Thanks to @ R _ 3X last night for a huge raid....</td>\n",
       "      <td>thanks r x last night huge raid made night</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@Borderlands. Can we please have a big hot fix...</td>\n",
       "      <td>borderland please big hot fix basic factory ch...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@amazon @amazonIN @AmazonHelp I purchased pre ...</td>\n",
       "      <td>amazon amazonin amazonhelp purchased pre booke...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Yo&lt;unk&gt; headed to top 5 S5 Watson kills &lt;unk&gt;</td>\n",
       "      <td>yo unk headed top watson kill unk</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>making hoes mad in CSGO since 2014 pic.twitter...</td>\n",
       "      <td>making hoe mad csgo since pic twitter com ia q...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.... Read More to Here :. en cnweekly.... com ...</td>\n",
       "      <td>read en cnweekly com national news state sat</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Borderlands 3 Ps4 the Anointed Fox 3 Pistol. T...</td>\n",
       "      <td>borderland p anointed fox pistol stuff dlvr rpl</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>accidental tk apology 3 piece</td>\n",
       "      <td>accidental tk apology piece</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Just drop off the correct token on market upda...</td>\n",
       "      <td>drop correct token market update nba k myteam ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>With tricks from FB they have to be regulated....</td>\n",
       "      <td>trick fb regulated people believe everything s...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bro @ NBA2K why tf 2K21 demo is worse than the...</td>\n",
       "      <td>bro nba k tf k demo worse whole game k</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VACCINE PAUSED: Johnson &amp; Johnson further paus...</td>\n",
       "      <td>vaccine paused johnson johnson paused covid va...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>do i really want more call of duty finsubs... ...</td>\n",
       "      <td>really want call duty finsubs unk little fun f...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NVIDIA SHIELD UPDATE - NEW FEATURE ! This is A...</td>\n",
       "      <td>nvidia shield update new feature awesome docsq...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>From Me : That hasn ’ t watched a whole single...</td>\n",
       "      <td>watched whole single game season also wtf siak...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@Ronnie2K @NBA2K - I was just about to win my ...</td>\n",
       "      <td>ronnie k nba k win fourth game rush point grad...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>This ar switch definitely saved the squad, bur...</td>\n",
       "      <td>ar switch definitely saved squad burst ftw</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>It only took them 16, 91 000 million lawsuits lol</td>\n",
       "      <td>took million lawsuit lol</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Great piece of music from the futbal legend. T...</td>\n",
       "      <td>great piece music futbal legend african win fi...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@GoI_MeitY whenever those PUBG numbers are run...</td>\n",
       "      <td>goi meity whenever pubg number running high in...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   I’m just gonna say it - Overwatch game being n...   \n",
       "1   A company that has the ability to design and p...   \n",
       "2                Everything is better when it's black   \n",
       "3                                   RIP Battlefield V   \n",
       "4   Borderlands 3 Chapter 1 youtu.be / 0SKu6Vr4iXU...   \n",
       "5   johnson & johnson about a purchase COVID but s...   \n",
       "6   The World<unk> Warcraft category on play is so...   \n",
       "7   @joem135... You know what's bloody stupid? Som...   \n",
       "8   from me is slightly disappointed being my sche...   \n",
       "9   @ Xfinity The Call of Duty: Black Ops Cold War...   \n",
       "10  Thanks to @ R _ 3X last night for a huge raid....   \n",
       "11  @Borderlands. Can we please have a big hot fix...   \n",
       "12  @amazon @amazonIN @AmazonHelp I purchased pre ...   \n",
       "13      Yo<unk> headed to top 5 S5 Watson kills <unk>   \n",
       "14  making hoes mad in CSGO since 2014 pic.twitter...   \n",
       "15  .... Read More to Here :. en cnweekly.... com ...   \n",
       "16  Borderlands 3 Ps4 the Anointed Fox 3 Pistol. T...   \n",
       "17                      accidental tk apology 3 piece   \n",
       "18  Just drop off the correct token on market upda...   \n",
       "19  With tricks from FB they have to be regulated....   \n",
       "20  Bro @ NBA2K why tf 2K21 demo is worse than the...   \n",
       "21  VACCINE PAUSED: Johnson & Johnson further paus...   \n",
       "22  do i really want more call of duty finsubs... ...   \n",
       "23  NVIDIA SHIELD UPDATE - NEW FEATURE ! This is A...   \n",
       "24  From Me : That hasn ’ t watched a whole single...   \n",
       "25  @Ronnie2K @NBA2K - I was just about to win my ...   \n",
       "26  This ar switch definitely saved the squad, bur...   \n",
       "27  It only took them 16, 91 000 million lawsuits lol   \n",
       "28  Great piece of music from the futbal legend. T...   \n",
       "29  @GoI_MeitY whenever those PUBG numbers are run...   \n",
       "\n",
       "                              tweet_text_preprocessed   sentiment  \n",
       "0   gonna say overwatch game nominated lgbtq game ...    Negative  \n",
       "1   company ability design produce top line graphi...    Negative  \n",
       "2                             everything better black  Irrelevant  \n",
       "3                                   rip battlefield v    Negative  \n",
       "4   borderland chapter youtu sku vr ixu via youtub...     Neutral  \n",
       "5            johnson johnson purchase covid still fix    Negative  \n",
       "6   world unk warcraft category play sooo congeste...  Irrelevant  \n",
       "7   joem know bloody stupid someone stupid profit ...  Irrelevant  \n",
       "8   slightly disappointed schedule explicitly appr...     Neutral  \n",
       "9   xfinity call duty black ops cold war open beta...    Negative  \n",
       "10         thanks r x last night huge raid made night  Irrelevant  \n",
       "11  borderland please big hot fix basic factory ch...    Negative  \n",
       "12  amazon amazonin amazonhelp purchased pre booke...    Negative  \n",
       "13                  yo unk headed top watson kill unk     Neutral  \n",
       "14  making hoe mad csgo since pic twitter com ia q...     Neutral  \n",
       "15       read en cnweekly com national news state sat     Neutral  \n",
       "16    borderland p anointed fox pistol stuff dlvr rpl    Positive  \n",
       "17                        accidental tk apology piece     Neutral  \n",
       "18  drop correct token market update nba k myteam ...    Negative  \n",
       "19  trick fb regulated people believe everything s...    Negative  \n",
       "20             bro nba k tf k demo worse whole game k    Negative  \n",
       "21  vaccine paused johnson johnson paused covid va...     Neutral  \n",
       "22  really want call duty finsubs unk little fun f...    Positive  \n",
       "23  nvidia shield update new feature awesome docsq...    Positive  \n",
       "24  watched whole single game season also wtf siak...  Irrelevant  \n",
       "25  ronnie k nba k win fourth game rush point grad...    Negative  \n",
       "26         ar switch definitely saved squad burst ftw    Positive  \n",
       "27                           took million lawsuit lol    Negative  \n",
       "28  great piece music futbal legend african win fi...  Irrelevant  \n",
       "29  goi meity whenever pubg number running high in...    Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('tweets_processed.csv')[['tweet_text','tweet_text_preprocessed', 'sentiment']].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175cb146",
   "metadata": {},
   "source": [
    "### 2.4 Initialize Sagemaker ScriptProcessor (SKLearnProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb0b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee772ef",
   "metadata": {},
   "source": [
    "### 2.5 (TODO) Create S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7895cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e9e58",
   "metadata": {},
   "source": [
    "### 2.6 Copy tweets.csv to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9efc04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./tweets.csv to s3://dsml-chrisi-bucket/workshop/tweets.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tweets.csv s3://dsml-chrisi-bucket/workshop/tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10e99a",
   "metadata": {},
   "source": [
    "### 2.7 Run ScriptProcessor\n",
    "IMPORTANT: Uncomment/comment lines in `process_tweets.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "008a5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2022-03-31-04-37-27-621\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://dsml-chrisi-bucket/workshop/tweets.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-995383923238/sagemaker-scikit-learn-2022-03-31-04-37-27-621/input/code/process_tweets.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'tweet_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://dsml-chrisi-bucket/workshop', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /miniconda3/lib/python3.7/site-packages (from nltk) (8.0.3)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /miniconda3/lib/python3.7/site-packages (from nltk) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /miniconda3/lib/python3.7/site-packages (from click->nltk) (4.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.6.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, joblib, nltk\u001b[0m\n",
      "\u001b[34mSuccessfully installed joblib-1.1.0 nltk-3.7 regex-2022.3.15\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package wordnet to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/wordnet.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package stopwords to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/stopwords.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/omw-1.4.zip.\u001b[0m\n",
      "\n",
      "CPU times: user 625 ms, sys: 40.2 ms, total: 666 ms\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='process_tweets.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source='s3://dsml-chrisi-bucket/workshop/tweets.csv',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='tweet_output',\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination='s3://dsml-chrisi-bucket/workshop'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d5d17",
   "metadata": {},
   "source": [
    "### 2.8 Open a new browser tab and check your running job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de835c5b",
   "metadata": {},
   "source": [
    "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/processing-jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa3261",
   "metadata": {},
   "source": [
    "### 2.9 View output in S3 bucket\n",
    "You should see a new file named `tweets_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b238ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 04:37:27    1299233 tweets.csv\n",
      "2022-03-31 04:41:25    1830178 tweets_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639cdf2",
   "metadata": {},
   "source": [
    "### RECAP #1\n",
    "1. Saved dataframe to a csv file in S3 (2.1)\n",
    "2. Transferred notebook code into Python script file (2.2)\n",
    "3. Used Sagemaker ScriptProcessor to run our script in a container and process the files to/from S3 (2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c84ec5",
   "metadata": {},
   "source": [
    "### 3.0 Split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d5071",
   "metadata": {},
   "source": [
    "```python\n",
    "train_df, test_df = train_test_split(df,test_size = 0.05, random_state =42)\n",
    "\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_train = train_df['tweet_text_preprocessed']\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "X_test = test_df['tweet_text_preprocessed']\n",
    "y_test = test_df['sentiment']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b0333",
   "metadata": {},
   "source": [
    "### 3.1 Keras tokenization word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489a284",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "\n",
    "keras_tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "\n",
    "\n",
    "def tokenize_pad_sequences(tweet_text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    tweet_text = tokenizer.texts_to_sequences(tweet_text)\n",
    "    # Pad sequences to the same length\n",
    "    tweet_text = pad_sequences(tweet_text, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return tweet_text\n",
    "\n",
    "keras_tokenizer.fit_on_texts(train_df['tweet_text_preprocessed'])\n",
    "train_texts_to_sequences = keras_tokenizer.texts_to_sequences(train_df['tweet_text_preprocessed'])\n",
    "train_texts_to_sequences = pad_sequences(train_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
    "\n",
    "\n",
    "test_texts_to_sequences = keras_tokenizer.texts_to_sequences(test_df['tweet_text_preprocessed'])\n",
    "test_texts_to_sequences = pad_sequences(test_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
    "\n",
    "\n",
    "# saving\n",
    "# with open('keras_tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(keras_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "tokenizer_json =  keras_tokenizer.to_json()\n",
    "with io.open(f'{current_path}/keras_model_files/keras_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b48c6",
   "metadata": {},
   "source": [
    "### 3.2 Write above code into training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f17fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing keras_train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile keras_train_model.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model\n",
    "#from keras.models import save_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "keras_tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "\n",
    "\n",
    "def train(input_file, output_folder):\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df,test_size = 0.05, random_state =42)\n",
    "\n",
    "    train_df.reset_index(drop = True, inplace = True)\n",
    "    test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    X_train = train_df['tweet_text_preprocessed']\n",
    "    y_train = train_df['sentiment']\n",
    "\n",
    "    X_test = test_df['tweet_text_preprocessed']\n",
    "    y_test = test_df['sentiment']\n",
    "\n",
    "    keras_tokenizer.fit_on_texts(train_df['tweet_text_preprocessed'])\n",
    "    train_texts_to_sequences = keras_tokenizer.texts_to_sequences(train_df['tweet_text_preprocessed'])\n",
    "    train_texts_to_sequences = pad_sequences(train_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "    train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
    "\n",
    "    test_texts_to_sequences = keras_tokenizer.texts_to_sequences(test_df['tweet_text_preprocessed'])\n",
    "    test_texts_to_sequences = pad_sequences(test_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "    test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    embedding_vector_size = 16\n",
    "    lstm_units = 20\n",
    "    keras_model.add(Embedding(max_words,embedding_vector_size,input_length=max_len))\n",
    "    #keras_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "    keras_model.add(Bidirectional(LSTM(lstm_units)))\n",
    "    keras_model.add(Dense(4, activation='softmax'))\n",
    "    keras_model.compile(\n",
    "         loss='sparse_categorical_crossentropy',\n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "    \n",
    "    X_train_keras = train_texts_to_sequences\n",
    "    y_train_keras = train_df['sentiment_index']\n",
    "\n",
    "    X_test_keras = test_texts_to_sequences\n",
    "    y_test_keras = test_df['sentiment_index']\n",
    "\n",
    "    keras_model.fit(\n",
    "         X_train_keras, y_train_keras,\n",
    "         validation_data=(X_test_keras, y_test_keras),\n",
    "         epochs=1)\n",
    "    \n",
    "    save_model(keras_model, output_folder.rstrip('/') + '/', save_format='tf')\n",
    "\n",
    "    print('DONE')\n",
    "    #tokenizer_json =  keras_tokenizer.to_json()\n",
    "    #with io.open(f'{current_path}/keras_model_files/keras_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    #    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = os.environ.get('SM_CHANNEL_TRAIN') + '/tweets_processed.csv'\n",
    "    output_folder = os.environ.get('SM_MODEL_DIR') #os.path.join('/opt/ml/processing/output', 'tweets_processed.csv')\n",
    "\n",
    "    #input_file = os.path.join('.', 'tweets_processed.csv')\n",
    "    #output_folder = '.'\n",
    "    \n",
    "    print('Input', input_file)\n",
    "    print('Output', output_folder)\n",
    "       \n",
    "    train(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8a439",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Sagemaker Estimator\n",
    "Let's compare with (2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27978c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b5e5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "                 entry_point='keras_train_model.py',\n",
    "                 instance_type='ml.p3.2xlarge', #'ml.p3.8xlarge', 'local'\n",
    "                 instance_count=1,\n",
    "                 source_dir='.',\n",
    "                 role=role,\n",
    "                 framework_version='2.3.2',\n",
    "                 py_version='py37',\n",
    "                 output_path='s3://dsml-chrisi-bucket/workshop',\n",
    "                 hyperparameters={\n",
    "                     #'embedding': True,\n",
    "                     #'modelstart': 1,\n",
    "                     #'batch-size': 64,\n",
    "                     #'modelfinish': 5\n",
    "                 },\n",
    "                 #script_mode=True,\n",
    "                 #dependencies=dependencies,\n",
    "                 #image_uri=<image_uri>,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c785340",
   "metadata": {},
   "source": [
    "### 3.4 Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2d0bb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 05:07:07 Starting - Starting the training job...\n",
      "2022-03-31 05:07:33 Starting - Preparing the instances for trainingProfilerReport-1648703227: InProgress\n",
      ".........\n",
      "2022-03-31 05:09:06 Downloading - Downloading input data\n",
      "2022-03-31 05:09:06 Training - Downloading the training image...............\n",
      "2022-03-31 05:11:31 Training - Training image download completed. Training in progress.\u001b[34m2022-03-31 05:11:27.096939: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:27.103660: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:27.348119: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:27.437188: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:31,402 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:32,287 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tweets-chris04\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://dsml-chrisi-bucket/tweets-chris04/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"keras_train_model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"keras_train_model.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=keras_train_model.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=keras_train_model\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://dsml-chrisi-bucket/tweets-chris04/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tweets-chris04\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://dsml-chrisi-bucket/tweets-chris04/source/sourcedir.tar.gz\",\"module_name\":\"keras_train_model\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"keras_train_model.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 keras_train_model.py --model_dir s3://dsml-chrisi-bucket/workshop/tweets-chris01/model\u001b[0m\n",
      "\u001b[34mInput /opt/ml/input/data/train/tweets_processed.csv\u001b[0m\n",
      "\u001b[34mOutput /opt/ml/model\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:40.354 ip-10-0-198-174.us-west-2.compute.internal:35 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:40.741 ip-10-0-198-174.us-west-2.compute.internal:35 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.531 ip-10-0-198-174.us-west-2.compute.internal:35 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.531 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.532 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.532 ip-10-0-198-174.us-west-2.compute.internal:35 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.533 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:425] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m[2022-03-31 05:11:41.536 ip-10-0-198-174.us-west-2.compute.internal:35 INFO hook.py:425] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m#015  1/221 [..............................] - ETA: 0s - loss: 1.3868 - accuracy: 0.2812 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  6/221 [..............................] - ETA: 2s - loss: 1.3863 - accuracy: 0.2448 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 11/221 [>.............................] - ETA: 2s - loss: 1.3864 - accuracy: 0.2301 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 17/221 [=>............................] - ETA: 2s - loss: 1.3860 - accuracy: 0.2445 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 23/221 [==>...........................] - ETA: 1s - loss: 1.3850 - accuracy: 0.2704 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 28/221 [==>...........................] - ETA: 1s - loss: 1.3844 - accuracy: 0.2690 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 33/221 [===>..........................] - ETA: 1s - loss: 1.3830 - accuracy: 0.2794 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 38/221 [====>.........................] - ETA: 1s - loss: 1.3841 - accuracy: 0.2722 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 43/221 [====>.........................] - ETA: 1s - loss: 1.3846 - accuracy: 0.2638 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 48/221 [=====>........................] - ETA: 1s - loss: 1.3844 - accuracy: 0.2643 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 53/221 [======>.......................] - ETA: 1s - loss: 1.3842 - accuracy: 0.2618 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 58/221 [======>.......................] - ETA: 1s - loss: 1.3842 - accuracy: 0.2619 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 63/221 [=======>......................] - ETA: 1s - loss: 1.3838 - accuracy: 0.2659 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 67/221 [========>.....................] - ETA: 1s - loss: 1.3833 - accuracy: 0.2654 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 72/221 [========>.....................] - ETA: 1s - loss: 1.3831 - accuracy: 0.2639 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 77/221 [=========>....................] - ETA: 1s - loss: 1.3831 - accuracy: 0.2662 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 82/221 [==========>...................] - ETA: 1s - loss: 1.3826 - accuracy: 0.2717 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 88/221 [==========>...................] - ETA: 1s - loss: 1.3818 - accuracy: 0.2763 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 93/221 [===========>..................] - ETA: 1s - loss: 1.3813 - accuracy: 0.2802 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 99/221 [============>.................] - ETA: 1s - loss: 1.3808 - accuracy: 0.2869 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015104/221 [=============>................] - ETA: 1s - loss: 1.3803 - accuracy: 0.2900 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015109/221 [=============>................] - ETA: 1s - loss: 1.3793 - accuracy: 0.2924 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015114/221 [==============>...............] - ETA: 1s - loss: 1.3787 - accuracy: 0.2966 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015119/221 [===============>..............] - ETA: 1s - loss: 1.3774 - accuracy: 0.2994 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015124/221 [===============>..............] - ETA: 1s - loss: 1.3765 - accuracy: 0.3012 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015129/221 [================>.............] - ETA: 0s - loss: 1.3753 - accuracy: 0.3067 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015135/221 [=================>............] - ETA: 0s - loss: 1.3745 - accuracy: 0.3083 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015141/221 [==================>...........] - ETA: 0s - loss: 1.3730 - accuracy: 0.3107 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015147/221 [==================>...........] - ETA: 0s - loss: 1.3708 - accuracy: 0.3163 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015152/221 [===================>..........] - ETA: 0s - loss: 1.3690 - accuracy: 0.3207 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015157/221 [====================>.........] - ETA: 0s - loss: 1.3666 - accuracy: 0.3244 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015162/221 [====================>.........] - ETA: 0s - loss: 1.3629 - accuracy: 0.3285 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015168/221 [=====================>........] - ETA: 0s - loss: 1.3616 - accuracy: 0.3322 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015173/221 [======================>.......] - ETA: 0s - loss: 1.3594 - accuracy: 0.3363 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015179/221 [=======================>......] - ETA: 0s - loss: 1.3552 - accuracy: 0.3417 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015184/221 [=======================>......] - ETA: 0s - loss: 1.3528 - accuracy: 0.3448 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015189/221 [========================>.....] - ETA: 0s - loss: 1.3504 - accuracy: 0.3492 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015194/221 [=========================>....] - ETA: 0s - loss: 1.3484 - accuracy: 0.3507 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015200/221 [==========================>...] - ETA: 0s - loss: 1.3447 - accuracy: 0.3531 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015206/221 [==========================>...] - ETA: 0s - loss: 1.3412 - accuracy: 0.3565 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015211/221 [===========================>..] - ETA: 0s - loss: 1.3383 - accuracy: 0.3589 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015216/221 [============================>.] - ETA: 0s - loss: 1.3370 - accuracy: 0.3600 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015221/221 [==============================] - ETA: 0s - loss: 1.3332 - accuracy: 0.3630 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015221/221 [==============================] - 3s 15ms/step - loss: 1.3332 - accuracy: 0.3630 - batch: 0.0000e+00 - val_loss: 1.2142 - val_accuracy: 0.4853\u001b[0m\n",
      "\u001b[34mDONE\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:33.797253: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:33.797418: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:33.842099: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mkeras_train_model.py:42: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\u001b[0m\n",
      "\u001b[34mkeras_train_model.py:46: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:11:57.606515: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34m2022-03-31 05:12:07,207 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving containers. The SavedModel bundle is under directory \"model\", not a numeric name.\u001b[0m\n",
      "\u001b[34m2022-03-31 05:12:07,208 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-03-31 05:12:31 Uploading - Uploading generated training model\n",
      "2022-03-31 05:12:31 Completed - Training job completed\n",
      "Training seconds: 218\n",
      "Billable seconds: 218\n",
      "CPU times: user 1.29 s, sys: 51.2 ms, total: 1.34 s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({\n",
    "    'train': 's3://dsml-chrisi-bucket/workshop/',\n",
    "}, job_name='tweets-chris04')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13eb39",
   "metadata": {},
   "source": [
    "### 3.5 Open a new browser tab and check your training job\n",
    "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79b9ef",
   "metadata": {},
   "source": [
    "### 3.6 View output in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "357fb280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE tweets-chris04/\n",
      "2022-03-31 04:37:27    1299233 tweets.csv\n",
      "2022-03-31 04:41:25    1830178 tweets_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa95bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE debug-output/\n",
      "                           PRE output/\n",
      "                           PRE profiler-output/\n",
      "                           PRE rule-output/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/tweets-chris04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e738aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 05:12:17    1224576 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dsml-chrisi-bucket/workshop/tweets-chris04/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d45df7",
   "metadata": {},
   "source": [
    "### 3.7 Copy model.tar.gz to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bee9346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dsml-chrisi-bucket/workshop/tweets-chris04/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://dsml-chrisi-bucket/workshop/tweets-chris04/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67c61bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets/\n",
      "saved_model.pb\n",
      "variables/\n",
      "variables/variables.index\n",
      "variables/variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!mkdir keras_model_files\n",
    "!tar -xvf model.tar.gz --directory ./keras_model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea49c9",
   "metadata": {},
   "source": [
    "### RECAP #2\n",
    "1. Transferred notebook code into Python training script file (3.2)\n",
    "2. Used Sagemaker Estimator to run our script in a container and process the files to/from S3 (3.3 - 3.4)\n",
    "3. Copied back the Estimator output from S3 to local (3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8f526",
   "metadata": {},
   "source": [
    "### What next?\n",
    "- [Sagemaker notebook examples](https://github.com/aws/amazon-sagemaker-examples) - (accessible via the Sagemaker notebook extension on the lower left of this Jupyterlab)\n",
    "- [Data Science on AWS (Book)](https://www.oreilly.com/library/view/data-science-on/9781492079385/) \n",
    "- Non-AWS\n",
    "  - [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/quickstart/)\n",
    "  - [MLFlow](https://mlflow.org/)\n",
    "  - [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-hello-world)\n",
    "  - Tensorflow Extended\n",
    "- Moar AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7227b4f",
   "metadata": {},
   "source": [
    "![](images/AWLMLStack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce825665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
