{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bf9f84",
   "metadata": {},
   "source": [
    "# Using Sagemaker ScriptProcessors and Estimators\n",
    "### Preprocess data and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed5075",
   "metadata": {},
   "source": [
    "![](images/Processing-1.png)\n",
    "\n",
    "[Source](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90bab9",
   "metadata": {},
   "source": [
    "### 0.1a Git clone the workshop repo in your Sagemaker instance\n",
    "You can run the command below in a Terminal inside Jupyter/JupyterLab, or in a Jupyter notebook cell (prefix with `!`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bde7ab",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/ConcurDataScience/ConcurMLWorkshop.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ff879",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/Terminal.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8ee44",
   "metadata": {},
   "source": [
    "### 0.1b Upload/unzip sentiment-modularized.tar.gz to your Sagemaker notebook instance\n",
    "After uploading the sentiment-modularized.tar.gz file to your Sagemaker notebook instance, open a Terminal in JupyterLab and unzip the tar.gz file using the command below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ebdf8",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd Sagemaker && tar -xvf sentiment-modularized.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace62f8",
   "metadata": {},
   "source": [
    "You should see the following after unzipping the tar file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d951f8",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/unzipped-list.png\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e885d4d",
   "metadata": {},
   "source": [
    "### 0.2 Check if your bucket exists\n",
    "https://s3.console.aws.amazon.com/s3/buckets?region=us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c40678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'dsml-chrisi-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d60af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://{BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5358d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31aae2",
   "metadata": {},
   "source": [
    "If your bucket does not exist, you can run the cell below to create it (remove comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 mb s3://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023d40a",
   "metadata": {},
   "source": [
    "### 0.3 Create the `workshop` and `athena_log` folders in your S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c312d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: resources/dummy.txt to s3://dsml-chrisi-bucket/workshop/dummy.txt\n",
      "upload: resources/dummy.txt to s3://dsml-chrisi-bucket/athena_log/dummy.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp resources/dummy.txt s3://{BUCKET}/workshop/dummy.txt\n",
    "!aws s3 cp resources/dummy.txt s3://{BUCKET}/athena_log/dummy.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46eeffc",
   "metadata": {},
   "source": [
    "### 1.0 Get data via PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aff0399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyathena in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.2)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.21.25)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (8.0.1)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.24.25)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->pyathena) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35649bf",
   "metadata": {},
   "source": [
    "Check your database-table name here - https://us-west-2.console.aws.amazon.com/glue/home?region=us-west-2#catalog:tab=tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f4ac7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/table-database.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cc83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"enriched_data\"\n",
    "database = \"twitter-sentiment-analysis-db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2467865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "conn = connect(s3_staging_dir=f\"s3://{BUCKET}/athena_log/\", \n",
    "               region_name=\"us-west-2\")\n",
    "df = pd.read_sql_query(f\"\"\" SELECT * FROM \"{database}\".\"{table}\" \"\"\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb4cebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#df.to_csv('resources/tweets_start.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d68692",
   "metadata": {},
   "source": [
    "### 1.01 (OPTIONAL) If Athena above is not available...\n",
    "...you can load the dataframe manually from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633a82e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dp_unique_key</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2579</td>\n",
       "      <td>uq_id_1010</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I had to repaint another gun for Tiny Tina's c...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Borderlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12947</td>\n",
       "      <td>uq_id_10146</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Nvidia’s RTX 3080 is more exciting than PlaySt...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12959</td>\n",
       "      <td>uq_id_10217</td>\n",
       "      <td>Negative</td>\n",
       "      <td>the</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12964</td>\n",
       "      <td>uq_id_10246</td>\n",
       "      <td>Positive</td>\n",
       "      <td>This price is simply incredible.</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13007</td>\n",
       "      <td>uq_id_10501</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Can't wait!</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id dp_unique_key      target  \\\n",
       "0   2579    uq_id_1010  Irrelevant   \n",
       "1  12947   uq_id_10146     Neutral   \n",
       "2  12959   uq_id_10217    Negative   \n",
       "3  12964   uq_id_10246    Positive   \n",
       "4  13007   uq_id_10501    Positive   \n",
       "\n",
       "                                                text updated_date  \\\n",
       "0  I had to repaint another gun for Tiny Tina's c...   21-03-2022   \n",
       "1  Nvidia’s RTX 3080 is more exciting than PlaySt...   21-03-2022   \n",
       "2                                                the   21-03-2022   \n",
       "3                   This price is simply incredible.   21-03-2022   \n",
       "4                                        Can't wait!   21-03-2022   \n",
       "\n",
       "          entity  \n",
       "0    Borderlands  \n",
       "1  Xbox(Xseries)  \n",
       "2  Xbox(Xseries)  \n",
       "3  Xbox(Xseries)  \n",
       "4  Xbox(Xseries)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('resources/tweets_start.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0b9a3",
   "metadata": {},
   "source": [
    "### 1.1 Load DataSet and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80a8d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# rename columns\n",
    "df.columns = ['tweet_id', 'dp_unique_key', 'sentiment', 'tweet_text', 'updated_date', 'entity']\n",
    "\n",
    "#Define the indexing for each possible label in a dictionary\n",
    "class_to_index = {\"Neutral\":0, \"Irrelevant\":1, \"Negative\":2, \"Positive\": 3}\n",
    "\n",
    "#Creates a reverse dictionary\n",
    "index_to_class = dict((v,k) for k, v in class_to_index.items())\n",
    "\n",
    "#Creates lambda functions, applying the appropriate dictionary\n",
    "names_to_ids = lambda n: np.array([class_to_index.get(x) for x in n])\n",
    "ids_to_names = lambda n: np.array([index_to_class.get(x) for x in n])\n",
    "\n",
    "#Convert the \"Sentiment\" column into indexes\n",
    "df[\"sentiment_index\"] = names_to_ids(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4755d",
   "metadata": {},
   "source": [
    "### 1.2 Look at dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3f6e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>dp_unique_key</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74675</th>\n",
       "      <td>9436</td>\n",
       "      <td>uq_id_8371</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I like this new</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9475</td>\n",
       "      <td>uq_id_8601</td>\n",
       "      <td>Negative</td>\n",
       "      <td>killing ISIS and its @PlayOverwatch ‘s fault. ...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9544</td>\n",
       "      <td>uq_id_8994</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I maybe went a little overboard with the fanta...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9596</td>\n",
       "      <td>uq_id_9285</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Overwatch makes me die grrr</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>12905</td>\n",
       "      <td>uq_id_9900</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Xbox Series X graphics source code stolen and ...</td>\n",
       "      <td>21-03-2022</td>\n",
       "      <td>Xbox(Xseries)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id dp_unique_key sentiment  \\\n",
       "74675      9436    uq_id_8371  Positive   \n",
       "74676      9475    uq_id_8601  Negative   \n",
       "74677      9544    uq_id_8994  Positive   \n",
       "74678      9596    uq_id_9285  Positive   \n",
       "74679     12905    uq_id_9900   Neutral   \n",
       "\n",
       "                                              tweet_text updated_date  \\\n",
       "74675                                    I like this new   21-03-2022   \n",
       "74676  killing ISIS and its @PlayOverwatch ‘s fault. ...   21-03-2022   \n",
       "74677  I maybe went a little overboard with the fanta...   21-03-2022   \n",
       "74678                        Overwatch makes me die grrr   21-03-2022   \n",
       "74679  Xbox Series X graphics source code stolen and ...   21-03-2022   \n",
       "\n",
       "              entity  sentiment_index  \n",
       "74675      Overwatch                3  \n",
       "74676      Overwatch                2  \n",
       "74677      Overwatch                3  \n",
       "74678      Overwatch                3  \n",
       "74679  Xbox(Xseries)                0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf256f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id           74680\n",
       "dp_unique_key      74680\n",
       "sentiment          74680\n",
       "tweet_text         74680\n",
       "updated_date       74680\n",
       "entity             74680\n",
       "sentiment_index    74680\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc39a62",
   "metadata": {},
   "source": [
    "### 1.3 Filter out rows where tweet_text is NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9648442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiter data where tweet is present\n",
    "df = df[df.tweet_text.isnull()==False]\n",
    "df['tweet_text'] = df['tweet_text'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e0a4c",
   "metadata": {},
   "source": [
    "### 1.4 Stratified Sampling - For this session purposes lets restrict to 5000 records for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bff283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_each_class = 2000\n",
    "dfs_list = []\n",
    "for unique_sentiment in np.unique(df.sentiment):\n",
    "    df_sentiment = df[df.sentiment == unique_sentiment].sample(n=number_of_rows_each_class, random_state = 42)\n",
    "    dfs_list.append(df_sentiment)\n",
    "df = pd.concat(dfs_list)\n",
    "df = df.sample(frac=1, random_state = 42)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bd9c400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive      2000\n",
       "Irrelevant    2000\n",
       "Negative      2000\n",
       "Neutral       2000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33527ec3",
   "metadata": {},
   "source": [
    "### 2.0 Prepare dataset for modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877522f2",
   "metadata": {},
   "source": [
    "```python\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet_text):\n",
    "    tweet_text = re.sub('[^a-zA-Z]', ' ', tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = tweet_text.split()\n",
    "    tweet_text = [lemmatizer.lemmatize(word) for word in tweet_text if (not(word in set(stopwords))) & (len(word)>1) ]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "df['tweet_text_preprocessed'] = df['tweet_text'].progress_apply(preprocess_tweet)\n",
    "df = df[df.tweet_text_preprocessed.apply(lambda text: len(text.split())>1)]\n",
    "#df['tweet_tokenized'] = df['tweet_tokenized'].progress_apply(lemmatize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622957b",
   "metadata": {},
   "source": [
    "### 2.1 Save dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160d86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9e804",
   "metadata": {},
   "source": [
    "### 2.2 Write code above into script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af8ced6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting process_tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile process_tweets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "install('nltk')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet_text):\n",
    "    tweet_text = re.sub('[^a-zA-Z]', ' ', tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = tweet_text.split()\n",
    "    tweet_text = [lemmatizer.lemmatize(word) for word in tweet_text if not word in set(stopwords)]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['tweet_text_preprocessed'] = df.apply(lambda x: preprocess_tweet(x['tweet_text']), axis=1)\n",
    "    df = df[df.tweet_text_preprocessed.apply(lambda text: len(text.split())>1)]\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #input_file = os.path.join('/opt/ml/processing/input', 'tweets.csv')\n",
    "    #output_file = os.path.join('/opt/ml/processing/output', 'tweets_processed.csv')\n",
    "    input_file = os.path.join('.', 'tweets.csv')\n",
    "    output_file = os.path.join('.', 'tweets_processed.csv')\n",
    "    \n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d43ca0",
   "metadata": {},
   "source": [
    "### 2.3 Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b7ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python process_tweets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f87df67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_text_preprocessed</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I support @ whichuk's campaign to end price go...</td>\n",
       "      <td>support whichuk campaign end price gouging als...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In multiplayer mode, I can't see any player be...</td>\n",
       "      <td>multiplayer mode see player similar color cost...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My family bad..</td>\n",
       "      <td>family bad</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. It'r s Snow Much My Fun! Enter me to finally...</td>\n",
       "      <td>r snow much fun enter finally win amazon store...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought going after workers for safety whist...</td>\n",
       "      <td>thought going worker safety whistleblowing pro...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I did this trick couple months ago hahahaha. I...</td>\n",
       "      <td>trick couple month ago hahahaha dumb</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>One of the best tactical valorant players in M...</td>\n",
       "      <td>one best tactical valorant player middle east ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Microsoft: ‘carbon-negative’ by 2030 even for ...</td>\n",
       "      <td>microsoft carbon negative even supply chain an...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In multiplayer play, I can't see any player be...</td>\n",
       "      <td>multiplayer play see player similar color cost...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There are only 2 days left until the start of ...</td>\n",
       "      <td>day left start psl elisa viihde pubg autumn ch...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@UbisoftSupport @GhostRecon I just can't chang...</td>\n",
       "      <td>ubisoftsupport ghostrecon change select class ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>If the @ Rainbow6Game servers failed for every...</td>\n",
       "      <td>rainbow game server failed everyone else recei...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.. store. be playstation. com /</td>\n",
       "      <td>store playstation com</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>New Titan Fall to Xbox Live on Ebay UK Titanfa...</td>\n",
       "      <td>new titan fall xbox live ebay uk titanfall mic...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Are  . buff.ly/3jdEiGn https://t.co/CdCiSpUNOy</td>\n",
       "      <td>buff ly jdeign http co cdcispunoy</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   I support @ whichuk's campaign to end price go...   \n",
       "1   In multiplayer mode, I can't see any player be...   \n",
       "2                                     My family bad..   \n",
       "3   . It'r s Snow Much My Fun! Enter me to finally...   \n",
       "4   I thought going after workers for safety whist...   \n",
       "5   I did this trick couple months ago hahahaha. I...   \n",
       "6   One of the best tactical valorant players in M...   \n",
       "7   Microsoft: ‘carbon-negative’ by 2030 even for ...   \n",
       "8   In multiplayer play, I can't see any player be...   \n",
       "9   There are only 2 days left until the start of ...   \n",
       "10  @UbisoftSupport @GhostRecon I just can't chang...   \n",
       "11  If the @ Rainbow6Game servers failed for every...   \n",
       "12                    .. store. be playstation. com /   \n",
       "13  New Titan Fall to Xbox Live on Ebay UK Titanfa...   \n",
       "14     Are  . buff.ly/3jdEiGn https://t.co/CdCiSpUNOy   \n",
       "\n",
       "                              tweet_text_preprocessed   sentiment  \n",
       "0   support whichuk campaign end price gouging als...    Negative  \n",
       "1   multiplayer mode see player similar color cost...    Negative  \n",
       "2                                          family bad  Irrelevant  \n",
       "3   r snow much fun enter finally win amazon store...     Neutral  \n",
       "4   thought going worker safety whistleblowing pro...    Negative  \n",
       "5                trick couple month ago hahahaha dumb  Irrelevant  \n",
       "6   one best tactical valorant player middle east ...  Irrelevant  \n",
       "7   microsoft carbon negative even supply chain an...     Neutral  \n",
       "8   multiplayer play see player similar color cost...    Negative  \n",
       "9   day left start psl elisa viihde pubg autumn ch...  Irrelevant  \n",
       "10  ubisoftsupport ghostrecon change select class ...    Negative  \n",
       "11  rainbow game server failed everyone else recei...    Negative  \n",
       "12                              store playstation com     Neutral  \n",
       "13  new titan fall xbox live ebay uk titanfall mic...     Neutral  \n",
       "14                  buff ly jdeign http co cdcispunoy     Neutral  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('tweets_processed.csv')[['tweet_text','tweet_text_preprocessed', 'sentiment']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45a5eb",
   "metadata": {},
   "source": [
    "### 2.4 Initialize Sagemaker ScriptProcessor (SKLearnProcessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85070cd",
   "metadata": {},
   "source": [
    "![](images/Processing-1.png)\n",
    "\n",
    "[Source](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e9c6e",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3702aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b4f68",
   "metadata": {},
   "source": [
    "### 2.6 Copy tweets.csv to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fe62355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./tweets.csv to s3://dsml-chrisi-bucket/workshop/tweets.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tweets.csv s3://{BUCKET}/workshop/tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50ab5f",
   "metadata": {},
   "source": [
    "### 2.7 Run ScriptProcessor\n",
    "IMPORTANT: Uncomment/comment lines in `process_tweets.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4aae769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2022-04-03-02-32-46-236\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://dsml-chrisi-bucket/workshop/tweets.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-995383923238/sagemaker-scikit-learn-2022-04-03-02-32-46-236/input/code/process_tweets.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'tweet_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://dsml-chrisi-bucket/workshop', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..........................\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /miniconda3/lib/python3.7/site-packages (from nltk) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /miniconda3/lib/python3.7/site-packages (from nltk) (4.62.3)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /miniconda3/lib/python3.7/site-packages (from click->nltk) (4.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, joblib, nltk\u001b[0m\n",
      "\u001b[34mSuccessfully installed joblib-1.1.0 nltk-3.7 regex-2022.3.15\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package wordnet to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/wordnet.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package stopwords to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/stopwords.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/omw-1.4.zip.\u001b[0m\n",
      "\n",
      "CPU times: user 528 ms, sys: 31.4 ms, total: 559 ms\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='process_tweets.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f's3://{BUCKET}/workshop/tweets.csv',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='tweet_output',\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination=f's3://{BUCKET}/workshop'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87a83c",
   "metadata": {},
   "source": [
    "### 2.8 Open a new browser tab and check your running job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1717ea",
   "metadata": {},
   "source": [
    "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/processing-jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ce223",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/processing-job.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff4fda",
   "metadata": {},
   "source": [
    "### 2.9 View output in S3 bucket\n",
    "You should see a new file named `tweets_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fa53148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-03 01:53:53          4 dummy.txt\n",
      "2022-04-03 02:16:43    1315064 tweets.csv\n",
      "2022-04-03 02:37:03    1856856 tweets_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{BUCKET}/workshop/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9bb74",
   "metadata": {},
   "source": [
    "### RECAP #1\n",
    "1. Saved dataframe to a csv file in S3 (2.1)\n",
    "2. Transferred notebook code into Python script file (2.2)\n",
    "3. Used Sagemaker ScriptProcessor to run our script in a container and process the files to/from S3 (2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f42cd",
   "metadata": {},
   "source": [
    "### 3.0 Split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b2702",
   "metadata": {},
   "source": [
    "```python\n",
    "train_df, test_df = train_test_split(df,test_size = 0.05, random_state =42)\n",
    "\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_train = train_df['tweet_text_preprocessed']\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "X_test = test_df['tweet_text_preprocessed']\n",
    "y_test = test_df['sentiment']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a9144",
   "metadata": {},
   "source": [
    "### 3.1 Keras tokenization word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4019e4",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "\n",
    "keras_tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "\n",
    "\n",
    "def tokenize_pad_sequences(tweet_text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    tweet_text = tokenizer.texts_to_sequences(tweet_text)\n",
    "    # Pad sequences to the same length\n",
    "    tweet_text = pad_sequences(tweet_text, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return tweet_text\n",
    "\n",
    "keras_tokenizer.fit_on_texts(train_df['tweet_text_preprocessed'])\n",
    "train_texts_to_sequences = keras_tokenizer.texts_to_sequences(train_df['tweet_text_preprocessed'])\n",
    "train_texts_to_sequences = pad_sequences(train_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
    "\n",
    "\n",
    "test_texts_to_sequences = keras_tokenizer.texts_to_sequences(test_df['tweet_text_preprocessed'])\n",
    "test_texts_to_sequences = pad_sequences(test_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
    "\n",
    "\n",
    "# saving\n",
    "# with open('keras_tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(keras_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "tokenizer_json =  keras_tokenizer.to_json()\n",
    "with io.open(f'{current_path}/keras_model_files/keras_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7747c",
   "metadata": {},
   "source": [
    "### 3.2 Write above code into training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9d8caa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing keras_train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile keras_train_model.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model\n",
    "#from keras.models import save_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "max_words = 5000\n",
    "max_len=50\n",
    "keras_tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "\n",
    "\n",
    "def train(input_file, output_folder):\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df,test_size = 0.05, random_state =42)\n",
    "\n",
    "    train_df.reset_index(drop = True, inplace = True)\n",
    "    test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    X_train = train_df['tweet_text_preprocessed']\n",
    "    y_train = train_df['sentiment']\n",
    "\n",
    "    X_test = test_df['tweet_text_preprocessed']\n",
    "    y_test = test_df['sentiment']\n",
    "\n",
    "    keras_tokenizer.fit_on_texts(train_df['tweet_text_preprocessed'])\n",
    "    train_texts_to_sequences = keras_tokenizer.texts_to_sequences(train_df['tweet_text_preprocessed'])\n",
    "    train_texts_to_sequences = pad_sequences(train_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "    train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
    "\n",
    "    test_texts_to_sequences = keras_tokenizer.texts_to_sequences(test_df['tweet_text_preprocessed'])\n",
    "    test_texts_to_sequences = pad_sequences(test_texts_to_sequences, padding='post', maxlen=max_len)\n",
    "    test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    embedding_vector_size = 16\n",
    "    lstm_units = 20\n",
    "    keras_model.add(Embedding(max_words,embedding_vector_size,input_length=max_len))\n",
    "    #keras_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "    keras_model.add(Bidirectional(LSTM(lstm_units)))\n",
    "    keras_model.add(Dense(4, activation='softmax'))\n",
    "    keras_model.compile(\n",
    "         loss='sparse_categorical_crossentropy',\n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "    \n",
    "    X_train_keras = train_texts_to_sequences\n",
    "    y_train_keras = train_df['sentiment_index']\n",
    "\n",
    "    X_test_keras = test_texts_to_sequences\n",
    "    y_test_keras = test_df['sentiment_index']\n",
    "\n",
    "    keras_model.fit(\n",
    "         X_train_keras, y_train_keras,\n",
    "         validation_data=(X_test_keras, y_test_keras),\n",
    "         epochs=1)\n",
    "    \n",
    "    save_model(keras_model, output_folder.rstrip('/') + '/', save_format='tf')\n",
    "\n",
    "    print('DONE')\n",
    "    #tokenizer_json =  keras_tokenizer.to_json()\n",
    "    #with io.open(f'{current_path}/keras_model_files/keras_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    #    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = os.environ.get('SM_CHANNEL_TRAIN') + '/tweets_processed.csv'\n",
    "    output_folder = os.environ.get('SM_MODEL_DIR') #os.path.join('/opt/ml/processing/output', 'tweets_processed.csv')\n",
    "\n",
    "    #input_file = os.path.join('.', 'tweets_processed.csv')\n",
    "    #output_folder = '.'\n",
    "    \n",
    "    print('Input', input_file)\n",
    "    print('Output', output_folder)\n",
    "       \n",
    "    train(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410cd1a",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Sagemaker Estimator\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html\n",
    "\n",
    "Let's compare with (2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0536d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47e817",
   "metadata": {},
   "source": [
    "DL Container images - https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70833d0d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/dl-containers.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a7586",
   "metadata": {},
   "source": [
    "Pricing - https://aws.amazon.com/sagemaker/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f96f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "                 entry_point='keras_train_model.py',\n",
    "                 instance_type='local', #'ml.c5.xlarge'', #'ml.p3.8xlarge', 'local'\n",
    "                 instance_count=1,\n",
    "                 source_dir='.',\n",
    "                 role=role,\n",
    "                 framework_version='2.3.2',\n",
    "                 py_version='py37',\n",
    "                 output_path=f's3://{BUCKET}/workshop',\n",
    "                 hyperparameters={\n",
    "                     #'embedding': True,\n",
    "                     #'modelstart': 1,\n",
    "                     #'batch-size': 64,\n",
    "                     #'modelfinish': 5\n",
    "                 },\n",
    "                 script_mode=True,\n",
    "                 #dependencies=dependencies,\n",
    "                 #image_uri=<image_uri>,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbee83d",
   "metadata": {},
   "source": [
    "### 3.4 Run training!\n",
    "Run both in `local` and non-local. For local, open up a Terminal and `docker ps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60cb5d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bxgk9e4pdq-algo-1-k1b80 ... \n",
      "Creating bxgk9e4pdq-algo-1-k1b80 ... done\n",
      "Attaching to bxgk9e4pdq-algo-1-k1b80\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:32.083693: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:32.083858: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:32.115200: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:33,538 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:33,545 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:35,376 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:35,396 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:35,415 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:35,428 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Training Env:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m {\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     },\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"current_host\": \"algo-1-k1b80\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         \"algo-1-k1b80\"\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     ],\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         \"model_dir\": \"s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-04-17-06-33-307/model\"\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     },\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         \"train\": {\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         }\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     },\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"job_name\": \"tensorflow-training-2022-04-04-17-06-33-307\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"master_hostname\": \"algo-1-k1b80\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"module_dir\": \"s3://dsml-chrisi-bucket/tensorflow-training-2022-04-04-17-06-33-307/source/sourcedir.tar.gz\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"module_name\": \"keras_train_model\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         \"current_host\": \"algo-1-k1b80\",\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m             \"algo-1-k1b80\"\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m         ]\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     },\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m     \"user_entry_point\": \"keras_train_model.py\"\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m }\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Environment variables:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_HOSTS=[\"algo-1-k1b80\"]\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_HPS={\"model_dir\":\"s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-04-17-06-33-307/model\"}\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_USER_ENTRY_POINT=keras_train_model.py\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-k1b80\",\"hosts\":[\"algo-1-k1b80\"]}\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_INPUT_DATA_CONFIG={\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_CHANNELS=[\"train\"]\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_CURRENT_HOST=algo-1-k1b80\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_MODULE_NAME=keras_train_model\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_MODULE_DIR=s3://dsml-chrisi-bucket/tensorflow-training-2022-04-04-17-06-33-307/source/sourcedir.tar.gz\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-k1b80\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-k1b80\"],\"hyperparameters\":{\"model_dir\":\"s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-04-17-06-33-307/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2022-04-04-17-06-33-307\",\"log_level\":20,\"master_hostname\":\"algo-1-k1b80\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://dsml-chrisi-bucket/tensorflow-training-2022-04-04-17-06-33-307/source/sourcedir.tar.gz\",\"module_name\":\"keras_train_model\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-k1b80\",\"hosts\":[\"algo-1-k1b80\"]},\"user_entry_point\":\"keras_train_model.py\"}\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_USER_ARGS=[\"--model_dir\",\"s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-04-17-06-33-307/model\"]\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m SM_HP_MODEL_DIR=s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-04-17-06-33-307/model\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m /usr/local/bin/python3.7 keras_train_model.py --model_dir s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-04-17-06-33-307/model\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Input /opt/ml/input/data/train/tweets_processed.csv\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Output /opt/ml/model\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m [2022-04-04 17:07:38.034 386d0b47f674:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m [2022-04-04 17:07:38.281 386d0b47f674:32 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 1.3175 - accuracy: 0.3707 - val_loss: 1.1633 - val_accuracy: 0.5053\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m DONE\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:36.282444: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:36.282591: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:36.314008: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m keras_train_model.py:42: SettingWithCopyWarning: \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m   train_df['tweet_keras_tokenized'] = list(train_texts_to_sequences)\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m keras_train_model.py:46: SettingWithCopyWarning: \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m   test_df['tweet_keras_tokenized'] = list(test_texts_to_sequences)\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Instructions for updating:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Instructions for updating:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:07:55.727437: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Instructions for updating:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m Instructions for updating:\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m INFO:tensorflow:Assets written to: /opt/ml/model/assets\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m INFO:tensorflow:Assets written to: /opt/ml/model/assets\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m \n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:08:03,842 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving containers. The SavedModel bundle is under directory \"model\", not a numeric name.\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 |\u001b[0m 2022-04-04 17:08:03,842 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mbxgk9e4pdq-algo-1-k1b80 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n",
      "CPU times: user 2.21 s, sys: 146 ms, total: 2.36 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'train': f's3://{BUCKET}/workshop/'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd776a",
   "metadata": {},
   "source": [
    "### 3.5 Open a new browser tab and check your training job\n",
    "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d75a7b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/training-job.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b97b08",
   "metadata": {},
   "source": [
    "### 3.6 View output in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79a95e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE tensorflow-training-2022-04-03-02-41-22-638/\n",
      "2022-04-03 01:53:53          4 dummy.txt\n",
      "2022-04-03 02:16:43    1315064 tweets.csv\n",
      "2022-04-03 02:37:03    1856856 tweets_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{BUCKET}/workshop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16d65463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-03 02:46:15    1215049 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{BUCKET}/workshop/tensorflow-training-2022-04-03-02-41-22-638/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4453c",
   "metadata": {},
   "source": [
    "### 3.7 Copy model.tar.gz to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93c44129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dsml-chrisi-bucket/workshop/tensorflow-training-2022-04-03-02-41-22-638/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/workshop/tensorflow-training-2022-04-03-02-41-22-638/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f57de296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\n",
      "assets/\n",
      "variables/\n",
      "variables/variables.index\n",
      "variables/variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!mkdir keras_model_files\n",
    "!tar -xvf model.tar.gz --directory ./keras_model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269c8f6",
   "metadata": {},
   "source": [
    "### RECAP #2\n",
    "1. Transferred notebook code into Python training script file (3.2)\n",
    "2. Used Sagemaker Estimator to run our script in a container and process the files to/from S3 (3.3 - 3.4)\n",
    "3. Copied back the Estimator output from S3 to local (3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea9f17",
   "metadata": {},
   "source": [
    "### What next?\n",
    "- [Sagemaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#)\n",
    "- [Sagemaker notebook examples](https://github.com/aws/amazon-sagemaker-examples) - (accessible via the Sagemaker notebook extension on the lower left of this Jupyterlab)\n",
    "- [Data Science on AWS (Book)](https://www.oreilly.com/library/view/data-science-on/9781492079385/) \n",
    "- Non-AWS\n",
    "  - [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/quickstart/)\n",
    "  - [MLFlow](https://mlflow.org/)\n",
    "  - [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-hello-world)\n",
    "  - Tensorflow Extended\n",
    "- Moar AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5237f",
   "metadata": {},
   "source": [
    "![](images/AWLMLStack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c8aa0",
   "metadata": {},
   "source": [
    "### (Example) Sagemaker Pipelines\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651030b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/pipeline-full.png\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2bdb3",
   "metadata": {},
   "source": [
    "```python\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=<role>,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"AbaloneProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=<input_data>, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "    ],\n",
    "    code=\"abalone/preprocessing.py\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c4d95",
   "metadata": {},
   "source": [
    "(Compare with 2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227d118",
   "metadata": {},
   "source": [
    "### END OF CLASS\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea32872",
   "metadata": {},
   "source": [
    "### Create html from notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91939a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook sentiment-classification-twitter-modularize.ipynb to html\n",
      "[NbConvertApp] Writing 700843 bytes to sentiment-classification-twitter-modularize.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html sentiment-classification-twitter-modularize.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c76d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    ".ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17e00c",
   "metadata": {},
   "source": [
    "### Zip up files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78aac438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm sentiment-modularized.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "350c427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./delete_resources.py\n",
      "./.gitignore\n",
      "./README.md\n",
      "./resources/\n",
      "./resources/tweets_start.csv\n",
      "./resources/process_tweets.py\n",
      "./resources/keras_train_model.py\n",
      "./resources/dummy.txt\n",
      "./sentiment-classification-twitter-modularize.html\n",
      "./images/\n",
      "./images/Processing-1.png\n",
      "./images/dl-containers.png\n",
      "./images/training-job.png\n",
      "./images/delete-bucket-01.png\n",
      "./images/stop-notebook.png\n",
      "./images/pipeline-full.png\n",
      "./images/unzipped-list.png\n",
      "./images/Terminal.png\n",
      "./images/processing-job.png\n",
      "./images/AWLMLStack.png\n",
      "./sentiment-classification-twitter-modularize.ipynb\n",
      "tar: .: file changed as we read it\n"
     ]
    }
   ],
   "source": [
    "!tar --exclude=\".ipynb_checkpoints*\" -zcvf sentiment-modularized.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf9795",
   "metadata": {},
   "source": [
    "### Delete stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf README.md images resources delete_resources.py sentiment-classification-twitter-modularize.html sentiment-modularized.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "447a8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf keras_model_files resources/keras_model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253c097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
