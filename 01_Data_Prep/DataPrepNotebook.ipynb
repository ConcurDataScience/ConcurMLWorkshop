{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2eedb6",
   "metadata": {},
   "source": [
    "### Lets Load basic spark and glue libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c033f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1648483062748_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-14-59.us-west-2.compute.internal:20888/proxy/application_1648483062748_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-15-222.us-west-2.compute.internal:8042/node/containerlogs/container_1648483062748_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62303b19",
   "metadata": {},
   "source": [
    "## Lets initiate a spark context and spark variable to do our big data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4046e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = SparkSession.builder.appName(\"index_create\").getOrCreate()\n",
    "job = Job(glueContext)\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b85fa",
   "metadata": {},
   "source": [
    "### Lets get the data downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c3359",
   "metadata": {},
   "source": [
    "``unzip data_prep_component.zip -d data_prep_component``\n",
    "\n",
    "``!aws s3 cp data_prep_component/ s3://datascience-ml-workshop-prep/data_prep_component/ --recursive``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a93c5",
   "metadata": {},
   "source": [
    "### Below are some of the utlity functions that we will be making use of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70228961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_latest(spark, bucket_name, source, upsert_or_delete):\n",
    "    prefix = str(source + '/' + upsert_or_delete+ '/')\n",
    "#     path ='s3://datascience-ml-workshop-prep/data_prep_component/upserts/03-21-2022'\n",
    "    path = get_most_recent_s3_object(bucket_name, prefix)\n",
    "    print(\"Currently Reading\", path)\n",
    "    df = spark.read.csv(path, header=True, sep='\\t')\n",
    "    df = df.drop('_c0')\n",
    "    return df\n",
    "\n",
    "def get_most_recent_s3_object(bucket_name,prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator( \"list_objects_v2\" )\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    latest = []\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            latest.append(max(page['Contents'], key=lambda x: x['LastModified']))\n",
    "    total_max = max(latest, key=lambda x: x['LastModified'])\n",
    "    latest_data_path = total_max['Key']\n",
    "    latest_data_path = \"/\".join(latest_data_path.split(\"/\")[:-1])\n",
    "    return str('s3://'+ bucket_name + '/' + latest_data_path)  \n",
    "\n",
    "def process_incrememtal_upserts(spark,delta_upserts, processed_data):\n",
    "    df = processed_data.unionByName(delta_upserts)\n",
    "    w = Window.partitionBy('dp_unique_key').orderBy(F.desc('updated_date'))\n",
    "    df = df.withColumn('Rank',F.dense_rank().over(w))\n",
    "    final_upsert_data = df.filter(df.Rank == 1).drop(df.Rank)\n",
    "    return final_upsert_data\n",
    "\n",
    "def process_first_upserts(spark,delta_upserts): \n",
    "    return delta_upserts\n",
    "    \n",
    "\n",
    "def process_incrememtal_deletes(spark, delta_deletes, processed_data):\n",
    "    if delta_deletes.count()>0:\n",
    "        data_post_delete_processing = processed_data.join(delta_deletes, 'dp_unique_key','left_anti')\n",
    "        return data_post_delete_processing\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def write_files(data, bucket_name, destination, script_type='processed_data'):\n",
    "    data.write.mode(\"overwrite\").csv(\"s3://\" + bucket_name+ \"/\" + destination + \"/tmp/\" + script_type + \"_tmp\", header=True, sep='\\t')\n",
    "    data =spark.read.csv(\"s3://\"+bucket_name + \"/\"+ destination +\"/tmp/\"+ script_type + \"_tmp\", header=True, sep='\\t')\n",
    "    data.write.mode(\"overwrite\").csv(\"s3://\" + bucket_name + \"/\" + destination +\"/\"+ script_type, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa3494",
   "metadata": {},
   "source": [
    "## Define some useful varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61baf2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket_name = 'datascience-ml-workshop-prep'\n",
    "source = 'data_prep_component'\n",
    "destination = 'labeling_data_component/data_prep_output'\n",
    "run = \"incremental\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a86bb",
   "metadata": {},
   "source": [
    "### Step 1: Lets load the DP data that we got today 21st of March! and process the upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426592bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/upserts/03-21-2022"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data/\", header=True, sep='\\t')\n",
    "except:\n",
    "    run=\"first\"\n",
    "delta_upserts = load_latest(spark, bucket_name, source,  'upserts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674c08a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing_first_run"
     ]
    }
   ],
   "source": [
    "if run==\"first\":\n",
    "    print(\"processing_first_run\")\n",
    "    final_data = process_first_upserts(spark, delta_upserts)\n",
    "else:\n",
    "    print(\"processing_incremental_run\")\n",
    "    final_data = process_incrememtal_upserts(spark,delta_upserts,processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c693cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1st batch upserts: 72004\n",
      "Count After 1st batch upserts is processed: 72004"
     ]
    }
   ],
   "source": [
    "print(\"Count of 1st batch upserts:\", delta_upserts.count())\n",
    "print(\"Count After 1st batch upserts is processed:\", final_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1201dfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "write_files(final_data, bucket_name, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a4c49",
   "metadata": {},
   "source": [
    "### Step2: Lets now process the deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689c814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/deletes/03-21-2022"
     ]
    }
   ],
   "source": [
    "delta_deletes = load_latest(spark, bucket_name, source,  'deletes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdb705c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "data_post_delete_processing = process_incrememtal_deletes(spark, delta_deletes, processed_data)\n",
    "if data_post_delete_processing is not None:\n",
    "        write_files(data_post_delete_processing, bucket_name, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f77d9539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1st batch Delete: 0\n",
      "Count After 1st batch upserts & Deletes are processed: 72004"
     ]
    }
   ],
   "source": [
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "print(\"Count of 1st batch Delete:\", delta_deletes.count())\n",
    "print(\"Count After 1st batch upserts & Deletes are processed:\", processed_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49108bbd",
   "metadata": {},
   "source": [
    "### Imagine now its tomorrow 22nd March, We get another batch of Upserts and Deletes, Lets try processing that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f36fcc",
   "metadata": {},
   "source": [
    "### But before that lets mimick Data Platform Api by running below  command in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55996a",
   "metadata": {},
   "source": [
    "``bash run_DP_API.sh``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d90a34",
   "metadata": {},
   "source": [
    "### Now lets do the same again  so that new data gets processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653ac00",
   "metadata": {},
   "source": [
    "### Lets process upserts first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88763662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/upserts/03-22-2022\n",
      "processing_incremental_run\n",
      "Count of 2nd batch upserts: 2690\n",
      "Count After 2nd batch upserts is processed: 74686"
     ]
    }
   ],
   "source": [
    "bucket_name = 'datascience-ml-workshop-prep'\n",
    "source = 'data_prep_component'\n",
    "destination = 'labeling_data_component/data_prep_output'\n",
    "run = \"incremental\"\n",
    "try:\n",
    "    processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data/\", header=True, sep='\\t')\n",
    "except:\n",
    "    run=\"first\"\n",
    "delta_upserts = load_latest(spark, bucket_name, source,  'upserts')\n",
    "if run==\"first\":\n",
    "    print(\"processing_first_run\")\n",
    "    final_data = process_first_upserts(spark, delta_upserts)\n",
    "else:\n",
    "    print(\"processing_incremental_run\")\n",
    "    final_data = process_incrememtal_upserts(spark,delta_upserts,processed_data)\n",
    "    \n",
    "print(\"Count of 2nd batch upserts:\", delta_upserts.count())\n",
    "print(\"Count After 2nd batch upserts is processed:\", final_data.count())\n",
    "write_files(final_data, bucket_name, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0bf8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    }
   ],
   "source": [
    "(72004+2690)-74686"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17246ec",
   "metadata": {},
   "source": [
    "### These were the 8 records that got updated out of 2690 total new updates( 2682 inserts +8 updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e6a05",
   "metadata": {},
   "source": [
    "### Lets prrocess deletes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb17ab1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/deletes/03-22-2022\n",
      "Count of 2nd batch Delete: 4\n",
      "Count After 2nd batch upserts & Deletes are processed: 74682"
     ]
    }
   ],
   "source": [
    "delta_deletes = load_latest(spark, bucket_name, source,  'deletes')\n",
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "data_post_delete_processing = process_incrememtal_deletes(spark, delta_deletes, processed_data)\n",
    "if data_post_delete_processing is not None:\n",
    "    write_files(data_post_delete_processing, bucket_name, destination)\n",
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "print(\"Count of 2nd batch Delete:\", delta_deletes.count())\n",
    "print(\"Count After 2nd batch upserts & Deletes are processed:\", processed_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d0688",
   "metadata": {},
   "source": [
    "### These were the 4 deletes that were processed(74686-4 = 74682)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419909c7",
   "metadata": {},
   "source": [
    "## Enrichment Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b7a9640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ddbc540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_part_2 = spark.read.csv('s3://'+bucket_name +'/'+ source + '/id_entity_mapper.csv',header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "456dbf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_part_1 = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e176f036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_data = data_part_1.join(data_part_2, ['Id','dp_unique_key'], 'inner').drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baecad80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74680"
     ]
    }
   ],
   "source": [
    "joined_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3712b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "write_files(joined_data, bucket_name, destination, 'enriched_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee950bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
