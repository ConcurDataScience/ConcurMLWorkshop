{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6c5499",
   "metadata": {},
   "source": [
    "### Lets Load basic spark and glue libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89befe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1648483062748_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-14-59.us-west-2.compute.internal:20888/proxy/application_1648483062748_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-21-82.us-west-2.compute.internal:8042/node/containerlogs/container_1648483062748_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75b828",
   "metadata": {},
   "source": [
    "### Overview :\n",
    "<img src=\"imgs/overview.png\" width=1000 height=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375a2ab",
   "metadata": {},
   "source": [
    "### Lets initiate a spark context and spark variable to do our big data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aff68dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = SparkSession.builder.appName(\"index_create\").getOrCreate()\n",
    "job = Job(glueContext)\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b1c8f",
   "metadata": {},
   "source": [
    "### Define some useful varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64791abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket_name = 'datascience-ml-workshop-prep' #  Pls Edit this, and this will be the bucket name you are keeping\n",
    "source = 'data_prep_component' # The folder we are reading from \n",
    "destination = 'labeling_data_component/data_prep_output' # The folder we will be writing to\n",
    "run = \"incremental\" # Default value of run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5e86a",
   "metadata": {},
   "source": [
    "### Lets get the data downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09765f3b",
   "metadata": {},
   "source": [
    "Lets open the terminal in new tab and download the data we will be using following commands one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2942cbb",
   "metadata": {},
   "source": [
    "`` sudo su``\n",
    "\n",
    "``curl --location --remote-header-name --remote-name https://github.com/ConcurDataScience/ConcurMLWorkshop/blob/main/01_Data_Prep/data_prep_component.zip``\n",
    "\n",
    "``unzip data_prep_component.zip -d data_prep_component``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe340f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'aws s3 cp data_prep_component/ s3://datascience-ml-workshop-prep/data_prep_component/ --recursive'"
     ]
    }
   ],
   "source": [
    "f\"aws s3 cp data_prep_component/ s3://{bucket_name}/data_prep_component/ --recursive\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16cf0f",
   "metadata": {},
   "source": [
    "### Below are some of the utlity functions that we will be making use of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145c83f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def load_latest(spark, bucket_name, source, upsert_or_delete):\n",
    "    \"\"\"\n",
    "        Description:The function identifies & Loads the latest object in S3 \n",
    "        and reads that latest object processes it, and stores it\n",
    "        Input: bucket_name, source, upserts or deletes\n",
    "        Output: the loaded dataframe\n",
    "    \"\"\"\n",
    "    prefix = str(source + '/' + upsert_or_delete+ '/')\n",
    "    path = get_most_recent_s3_object(bucket_name, prefix)\n",
    "    print(\"Currently Reading\", path)\n",
    "    df = spark.read.csv(path, header=True, sep='\\t')\n",
    "    df = df.drop('_c0')\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_most_recent_s3_object(bucket_name,prefix):\n",
    "    \"\"\"\n",
    "        Description:The function identifies the latest object in S3 \n",
    "        and passes back the latest objects URI\n",
    "        Input: bucket_name, prefix\n",
    "        Output: the latest object S3 URI\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator( \"list_objects_v2\" )\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    latest = []\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            latest.append(max(page['Contents'], key=lambda x: x['LastModified']))\n",
    "    total_max = max(latest, key=lambda x: x['LastModified'])\n",
    "    latest_data_path = total_max['Key']\n",
    "    latest_data_path = \"/\".join(latest_data_path.split(\"/\")[:-1])\n",
    "    return str('s3://'+ bucket_name + '/' + latest_data_path)  \n",
    "\n",
    "\n",
    "def process_incrememtal_upserts(spark,delta_upserts, processed_data):\n",
    "    \"\"\"\n",
    "        Description:The function parses the incremental upserts by unioning it and \n",
    "        then partioning by the unique key and sorted on updated_date\n",
    "        Input: delta_upserts and processed_Data\n",
    "        Output: the final updated dataset\n",
    "    \"\"\"\n",
    "    df = processed_data.unionByName(delta_upserts)\n",
    "    w = Window.partitionBy('dp_unique_key').orderBy(F.desc('updated_date'))\n",
    "    df = df.withColumn('Rank',F.dense_rank().over(w))\n",
    "    final_upsert_data = df.filter(df.Rank == 1).drop(df.Rank)\n",
    "    return final_upsert_data\n",
    "\n",
    "def process_first_upserts(spark,delta_upserts): \n",
    "    return delta_upserts\n",
    "    \n",
    "\n",
    "def process_incrememtal_deletes(spark, delta_deletes, processed_data):\n",
    "    \"\"\"\n",
    "        Description:The function parses the incremental deletes by doing a left anti join \n",
    "        Input: delta_deletes and processed_Data\n",
    "        Output: the final updated dataset\n",
    "    \"\"\"\n",
    "    if delta_deletes.count()>0:\n",
    "        data_post_delete_processing = processed_data.join(delta_deletes, 'dp_unique_key','left_anti')\n",
    "        return data_post_delete_processing\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def write_files(data, bucket_name, destination, script_type='processed_data'):\n",
    "    \"\"\"\n",
    "        Description:The function writes the data to the s3 location \n",
    "        Input: bucket_name, destination and type of script \n",
    "        Output: None\n",
    "    \"\"\"\n",
    "    data.write.mode(\"overwrite\").csv(\"s3://\" + bucket_name+ \"/\" + destination + \"/tmp/\" + script_type + \"_tmp\", header=True, sep='\\t')\n",
    "    data =spark.read.csv(\"s3://\"+bucket_name + \"/\"+ destination +\"/tmp/\"+ script_type + \"_tmp\", header=True, sep='\\t')\n",
    "    data.write.mode(\"overwrite\").csv(\"s3://\" + bucket_name + \"/\" + destination +\"/\"+ script_type, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7664cb2",
   "metadata": {},
   "source": [
    "### Step 1: Lets load the DP data that we got today 21st of March! and process the upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57898dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/upserts/03-21-2022"
     ]
    }
   ],
   "source": [
    "# We will first read our porcessed bucket to see what has been processed so far, \n",
    "# on an exception, it will mean that its a first run\n",
    "\n",
    "try:\n",
    "    processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data/\", header=True, sep='\\t')\n",
    "except:\n",
    "    run=\"first\"\n",
    "    \n",
    "# Then we will load the latest upsert data\n",
    "delta_upserts = load_latest(spark, bucket_name, source,  'upserts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f8da6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing_first_run"
     ]
    }
   ],
   "source": [
    "## Now if it was a first run, we will process process_first_upserts()\n",
    "#or if it is incremental we will process process_incremental_upserts()\n",
    "\n",
    "if run==\"first\":\n",
    "    print(\"processing_first_run\")\n",
    "    final_data = process_first_upserts(spark, delta_upserts)\n",
    "else:\n",
    "    print(\"processing_incremental_run\")\n",
    "    final_data = process_incrememtal_upserts(spark,delta_upserts,processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c43fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1st batch upserts: 72004\n",
      "Count After 1st batch upserts is processed: 72004"
     ]
    }
   ],
   "source": [
    "print(\"Count of 1st batch upserts:\", delta_upserts.count())\n",
    "print(\"Count After 1st batch upserts is processed:\", final_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a9c02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finally lets write the data as processed data\n",
    "write_files(final_data, bucket_name, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdfb0d",
   "metadata": {},
   "source": [
    "### Step2: Lets now process the deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbf32d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/deletes/03-21-2022"
     ]
    }
   ],
   "source": [
    "# Load the deletes\n",
    "delta_deletes = load_latest(spark, bucket_name, source,  'deletes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54f38206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the processed data and process deletes if any deletes are suppplied.\n",
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "data_post_delete_processing = process_incrememtal_deletes(spark, delta_deletes, processed_data)\n",
    "if data_post_delete_processing is not None:\n",
    "        write_files(data_post_delete_processing, bucket_name, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49f58943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1st batch Delete: 0\n",
      "Count After 1st batch upserts & Deletes are processed: 72004"
     ]
    }
   ],
   "source": [
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "print(\"Count of 1st batch Delete:\", delta_deletes.count())\n",
    "print(\"Count After 1st batch upserts & Deletes are processed:\", processed_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a9d3b",
   "metadata": {},
   "source": [
    "_**These are some of the rows from the Data Processed so far**_:\n",
    "<img src=\"imgs/un_updated.png\" width=1000 height=1000 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc99b3d",
   "metadata": {},
   "source": [
    "### Imagine now its tomorrow 22nd March, We get another batch of Upserts and Deletes, Lets try processing that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c722c",
   "metadata": {},
   "source": [
    "#### But before that lets mimick Data Platform Api by running below  command in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f2497",
   "metadata": {},
   "source": [
    "In the same terminal window that you had opened, try running below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ad4ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bash run_DP_API.sh datascience-ml-workshop-prep'"
     ]
    }
   ],
   "source": [
    "f\"bash run_DP_API.sh {bucket_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75c46b",
   "metadata": {},
   "source": [
    "#### Now lets do the same again  so that new data gets processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e736fb7",
   "metadata": {},
   "source": [
    "_**These rows got updated in todays run**_:\n",
    "<img src=\"imgs/after_update.png\" width=1000 height=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373758f",
   "metadata": {},
   "source": [
    "### Lets process upserts first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6ae418e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/upserts/03-22-2022\n",
      "processing_incremental_run\n",
      "Count of 2nd batch upserts: 2690\n",
      "Count After 2nd batch upserts is processed: 74686"
     ]
    }
   ],
   "source": [
    "run = \"incremental\"\n",
    "try:\n",
    "    processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data/\", header=True, sep='\\t')\n",
    "except:\n",
    "    run=\"first\"\n",
    "delta_upserts = load_latest(spark, bucket_name, source,  'upserts')\n",
    "if run==\"first\":\n",
    "    print(\"processing_first_run\")\n",
    "    final_data = process_first_upserts(spark, delta_upserts)\n",
    "else:\n",
    "    print(\"processing_incremental_run\")\n",
    "    final_data = process_incrememtal_upserts(spark,delta_upserts,processed_data)\n",
    "    \n",
    "print(\"Count of 2nd batch upserts:\", delta_upserts.count())\n",
    "print(\"Count After 2nd batch upserts is processed:\", final_data.count())\n",
    "write_files(final_data, bucket_name, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4b7bb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    }
   ],
   "source": [
    "(72004+2690)-74686"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf21d8",
   "metadata": {},
   "source": [
    "##### These were the 8 records that got updated out of 2690 total new updates( 2682 inserts +8 updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c323acd",
   "metadata": {},
   "source": [
    "_**These rows came in delete batch as they were identified to be malicious and containing PII**_:\n",
    "<img src=\"imgs/to_be_del.png\" width=1000 height=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc536c5",
   "metadata": {},
   "source": [
    "### Lets process deletes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97329d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Reading s3://datascience-ml-workshop-prep/data_prep_component/deletes/03-22-2022\n",
      "Count of 2nd batch Delete: 4\n",
      "Count After 2nd batch upserts & Deletes are processed: 74682"
     ]
    }
   ],
   "source": [
    "delta_deletes = load_latest(spark, bucket_name, source,  'deletes')\n",
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "data_post_delete_processing = process_incrememtal_deletes(spark, delta_deletes, processed_data)\n",
    "if data_post_delete_processing is not None:\n",
    "    write_files(data_post_delete_processing, bucket_name, destination)\n",
    "processed_data = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')\n",
    "print(\"Count of 2nd batch Delete:\", delta_deletes.count())\n",
    "print(\"Count After 2nd batch upserts & Deletes are processed:\", processed_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a429f",
   "metadata": {},
   "source": [
    "##### These were the 4 deletes that were processed(74686-4 = 74682)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fdb25",
   "metadata": {},
   "source": [
    "## Enrichment Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebc07dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c330ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'datascience-ml-workshop-prep' #  Pls Edit this, and this will be the bucket name you are keeping\n",
    "source = 'data_prep_component' # The folder we are reading from \n",
    "destination = 'labeling_data_component/data_prep_output' # The folder we will be writing to\n",
    "run = \"incremental\" # Default value of run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7badf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files(data, bucket_name, destination, script_type='processed_data'):\n",
    "    \"\"\"\n",
    "        Description:The function writes the data to the s3 location \n",
    "        Input: bucket_name, destination and type of script \n",
    "        Output: None\n",
    "    \"\"\"\n",
    "    data.write.mode(\"overwrite\").csv(\"s3://\" + bucket_name+ \"/\" + destination + \"/tmp/\" + script_type + \"_tmp\", header=True, sep='\\t')\n",
    "    data =spark.read.csv(\"s3://\"+bucket_name + \"/\"+ destination +\"/tmp/\"+ script_type + \"_tmp\", header=True, sep='\\t')\n",
    "    data.write.mode(\"overwrite\").csv(\"s3://\" + bucket_name + \"/\" + destination +\"/\"+ script_type, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea89c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_part_2 = spark.read.csv('s3://'+bucket_name +'/'+ source + '/id_entity_mapper.csv',header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a613d0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_part_1 = spark.read.csv(\"s3://\"+bucket_name+ \"/\"+destination+\"/processed_data\", header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74d9c65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_data = data_part_1.join(data_part_2, ['Id','dp_unique_key'], 'inner').drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "822f2d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74680"
     ]
    }
   ],
   "source": [
    "joined_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35417851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "write_files(joined_data, bucket_name, destination, 'enriched_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70dc4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
